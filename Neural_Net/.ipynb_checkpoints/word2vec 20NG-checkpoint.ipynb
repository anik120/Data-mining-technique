{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data = fetch_20newsgroups()\n",
    "\n",
    "print(\"Fecthing data..................................\")\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "newsgroups_data = newsgroups_train.data + newsgroups_test.data\n",
    "newsgroups_labels = list(newsgroups_train.target) + list(newsgroups_test.target)\n",
    "\n",
    "print(\"Data fetched successfully\")\n",
    "newsgroups_data_downsampled = []\n",
    "newsgroups_labels_downsampled = []\n",
    "for index in range(len(newsgroups_data)):\n",
    "    if newsgroups_labels[index] == 1 or newsgroups_labels[index] == 16:\n",
    "        newsgroups_data_downsampled.append(newsgroups_data[index])\n",
    "        newsgroups_labels_downsampled.append(newsgroups_labels[index])\n",
    "print(\"Data downsampled\")\n",
    "\n",
    "text = ' '.join(newsgroups_data_downsampled).lower()\n",
    "text[100:350]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_text = nltk.sent_tokenize(text)\n",
    "sentences = [nltk.word_tokenize(s) for s in sentences_text]\n",
    "temp_sentences = []\n",
    "for sentence in sentences:\n",
    "    temp_words = []\n",
    "    for word in sentence:\n",
    "        temp_word = re.sub('[^A-Za-z]+', '', word)\n",
    "        if temp_word != '':\n",
    "            temp_words.append(temp_word)\n",
    "    temp_sentences.append(temp_words)\n",
    "\n",
    "sentences = temp_sentences\n",
    "print(sentences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import  Counter\n",
    "from string import punctuation\n",
    "\n",
    "min_count = 5\n",
    "puncs = set(punctuation)\n",
    "\n",
    "\n",
    "flat_words = []\n",
    "for sentence in sentences:\n",
    "    flat_words += sentence\n",
    "    \n",
    "counts = Counter(list(flat_words))\n",
    "counts = pd.DataFrame(counts.most_common())\n",
    "counts.columns = ['word', 'count']\n",
    "\n",
    "counts = counts[counts['count'] >= min_count]\n",
    "counts = counts[~counts['word'].isin(puncs)]\n",
    "\n",
    "\n",
    "vocab = pd.Series(range(len(counts)), index=counts['word']).sort_index()\n",
    "\n",
    "print('The vocabulary has:', len(vocab), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence = [word for word in sentence if word in vocab.index]\n",
    "    if len(sentence):\n",
    "        filtered_sentences.append(sentence)\n",
    "sentences = filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(sentences):\n",
    "    sentences[i] = [vocab.loc[word] for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import skipgrams\n",
    "\n",
    "window_size = 1\n",
    "\n",
    "data = []\n",
    "for sentance in sentences:\n",
    "    data += skipgrams(sentance, 2, window_size)\n",
    "\n",
    "data = pd.DataFrame(data, columns=['x', 'y'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_size = 100000\n",
    "\n",
    "data_valid = data.iloc[-validation_size:]\n",
    "data_train = data.iloc[:-validation_size]\n",
    "print('Train size:', len(data_train), 'Validation size:', len(data_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = .01\n",
    "embed_size = 300\n",
    "batch_size = 64\n",
    "steps = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.int32, [None])\n",
    "targets = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform((len(vocab), embed_size), -1, 1))\n",
    "embed = tf.nn.embedding_lookup(embeddings, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(embed, len(vocab), activation=None,\n",
    "    kernel_initializer=tf.random_normal_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tf.one_hot(targets, len(vocab))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_batches(x, y, batch_size, n=None):\n",
    "    if n:\n",
    "        rand_start = np.random.randint(0, len(x) - batch_size * n)\n",
    "        x = x[rand_start:]\n",
    "        y = y[rand_start:]\n",
    "\n",
    "    for start in range(len(x))[::batch_size][:n]:\n",
    "        end = start + batch_size\n",
    "        yield x[start:end], y[start:end]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "while step < 20000:\n",
    "    start = time.time()\n",
    "    \n",
    "    # shuffle train data once in while\n",
    "    if step % 1000 == 0:\n",
    "        data_train = data_train.sample(frac=1.)\n",
    "    # train part\n",
    "    train_loss = []\n",
    "    \n",
    "    for x, y in get_batches(\n",
    "        data_train['x'].values, data_train['x'].values, batch_size, n=100):\n",
    "        step += 1\n",
    "        _, batch_loss = sess.run([train_op, loss], {inputs: x, targets: y})\n",
    "        print(batch_loss)\n",
    "        train_loss.append(batch_loss)\n",
    "            \n",
    "\n",
    "    # validation prat (one batch of \"validation_size\")\n",
    "    feed_dict = {inputs: data_valid['x'].values, targets: data_valid['x'].values}\n",
    "    valid_loss, x_vectors = sess.run([loss, embed], feed_dict)\n",
    "    y_vectors = sess.run(embed, {inputs: data_valid['x'].values})\n",
    "\n",
    "    # outputs\n",
    "    print('Step:', step, 'TLoss:', np.mean(train_loss), 'VLoss:', np.mean(valid_loss),\n",
    "          'Seconds %.1f' % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = sess.run(embeddings)\n",
    "vectors = pd.DataFrame(vectors, index=vocab.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = vocab.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "test_word = \"surveillance\"\n",
    "\n",
    "similarities = []\n",
    "for word in word_list:\n",
    "    similarities.append(cosine_similarity(vectors.loc[[test_word]], vectors.loc[[word]])[0][0])\n",
    "indices = np.array(similarities).argsort()[:(len(vocab))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in indices[:20]:\n",
    "    print(word_list[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def plot_with_labels(low_dim_embs, labels, filename):\n",
    "  assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "  plt.figure(figsize=(18, 18))  # in inches\n",
    "  for i, label in enumerate(labels):\n",
    "    x, y = low_dim_embs[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        xy=(x, y),\n",
    "        xytext=(5, 2),\n",
    "        textcoords='offset points',\n",
    "        ha='right',\n",
    "        va='bottom')\n",
    "\n",
    "  plt.savefig(filename)\n",
    "\n",
    "try:\n",
    "  from sklearn.manifold import TSNE\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  tsne = TSNE(\n",
    "      perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "  plot_only = 500\n",
    "  low_dim_embs = tsne.fit_transform(vectors)\n",
    "#   labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "  plot_with_labels(low_dim_embs, word_list, os.path.join('tsne.png'))\n",
    "\n",
    "except ImportError as ex:\n",
    "  print('Please install sklearn, matplotlib, and scipy to show embeddings.')\n",
    "  print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
