{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecthing data..................................\n",
      "Data fetched successfully\n",
      "Data downsampled\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ision\\nlines: 14\\ndistribution: world\\nnntp-posting-host: amber.ssd.csd.harris.com\\nx-newsreader: tin [version 1.1 pl9]\\n\\nrobert j.c. kyanko (rob@rjck.uucp) wrote:\\n> abraxis@iastate.edu writes in article <abraxis.734340159@class1.iastate.edu>:\\n> > anyone '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data = fetch_20newsgroups()\n",
    "\n",
    "print(\"Fecthing data..................................\")\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "newsgroups_data = newsgroups_train.data + newsgroups_test.data\n",
    "newsgroups_labels = list(newsgroups_train.target) + list(newsgroups_test.target)\n",
    "\n",
    "print(\"Data fetched successfully\")\n",
    "newsgroups_data_downsampled = []\n",
    "newsgroups_labels_downsampled = []\n",
    "for index in range(len(newsgroups_data)):\n",
    "    if newsgroups_labels[index] == 1 or newsgroups_labels[index] == 16:\n",
    "        newsgroups_data_downsampled.append(newsgroups_data[index])\n",
    "        newsgroups_labels_downsampled.append(newsgroups_labels[index])\n",
    "print(\"Data downsampled\")\n",
    "\n",
    "text = ' '.join(newsgroups_data_downsampled).lower()\n",
    "text[100:350]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'makes', 'the', 'right', 'of', 'the', 'people', 'to', 'keep', 'and', 'bear', 'many', 'modern', 'weapons', 'nonexistant']\n"
     ]
    }
   ],
   "source": [
    "sentences_text = nltk.sent_tokenize(text)\n",
    "sentences = [nltk.word_tokenize(s) for s in sentences_text]\n",
    "temp_sentences = []\n",
    "for sentence in sentences:\n",
    "    temp_words = []\n",
    "    for word in sentence:\n",
    "        temp_word = re.sub('[^A-Za-z]+', '', word)\n",
    "        if temp_word != '':\n",
    "            temp_words.append(temp_word)\n",
    "    temp_sentences.append(temp_words)\n",
    "\n",
    "sentences = temp_sentences\n",
    "print(sentences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary has: 4494 words\n"
     ]
    }
   ],
   "source": [
    "from collections import  Counter\n",
    "from string import punctuation\n",
    "\n",
    "min_count = 12\n",
    "puncs = set(punctuation)\n",
    "\n",
    "\n",
    "flat_words = []\n",
    "for sentence in sentences:\n",
    "    flat_words += sentence\n",
    "    \n",
    "counts = Counter(list(flat_words))\n",
    "counts = pd.DataFrame(counts.most_common())\n",
    "counts.columns = ['word', 'count']\n",
    "\n",
    "counts = counts[counts['count'] >= min_count]\n",
    "counts = counts[~counts['word'].isin(puncs)]\n",
    "\n",
    "\n",
    "vocab = pd.Series(range(len(counts)), index=counts['word']).sort_index()\n",
    "\n",
    "print('The vocabulary has:', len(vocab), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence = [word for word in sentence if word in vocab.index]\n",
    "    if len(sentence):\n",
    "        filtered_sentences.append(sentence)\n",
    "sentences = filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(sentences):\n",
    "    sentences[i] = [vocab.loc[word] for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>1405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>981</td>\n",
       "      <td>1405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>981</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1405</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      x     y\n",
       "0    12   981\n",
       "1    12  1405\n",
       "2   981  1405\n",
       "3   981    30\n",
       "4  1405    30"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import skipgrams\n",
    "\n",
    "window_size = 1\n",
    "\n",
    "data = []\n",
    "for sentance in sentences:\n",
    "    data += skipgrams(sentance, 2, window_size)\n",
    "\n",
    "data = pd.DataFrame(data, columns=['x', 'y'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>1405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>981</td>\n",
       "      <td>1405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>981</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1405</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1405</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>37</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>35</td>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>35</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1949</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1949</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>95</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>95</td>\n",
       "      <td>683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>240</td>\n",
       "      <td>683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>240</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>683</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>683</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>31</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       x     y\n",
       "0     12   981\n",
       "1     12  1405\n",
       "2    981  1405\n",
       "3    981    30\n",
       "4   1405    30\n",
       "5   1405    37\n",
       "6     30    37\n",
       "7     30   197\n",
       "8     37   197\n",
       "9     35  1949\n",
       "10    35    95\n",
       "11  1949    95\n",
       "12  1949   240\n",
       "13    95   240\n",
       "14    95   683\n",
       "15   240   683\n",
       "16   240    31\n",
       "17   683    31\n",
       "18   683   111\n",
       "19    31   111"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "880287"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 870287 Validation size: 10000\n"
     ]
    }
   ],
   "source": [
    "validation_size = 10000\n",
    "\n",
    "data_valid = data.iloc[-validation_size:]\n",
    "data_train = data.iloc[:-validation_size]\n",
    "print('Train size:', len(data_train), 'Validation size:', len(data_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "embed_size = 300\n",
    "batch_size = 64\n",
    "steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.int32, [None])\n",
    "targets = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform((len(vocab), embed_size), -1, 1))\n",
    "embed = tf.nn.embedding_lookup(embeddings, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(embed, len(vocab), activation=None,\n",
    "    kernel_initializer=tf.random_normal_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tf.one_hot(targets, len(vocab))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_batches(x, y, batch_size, n=None):\n",
    "    if n:\n",
    "        rand_start = np.random.randint(0, len(x) - batch_size * n)\n",
    "        x = x[rand_start:]\n",
    "        y = y[rand_start:]\n",
    "\n",
    "    for start in range(len(x))[::batch_size][:n]:\n",
    "        end = start + batch_size\n",
    "        yield x[start:end], y[start:end]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100 TLoss: 0.0 VLoss: 0.0 Seconds 7.3\n",
      "Step: 200 TLoss: 0.0 VLoss: 0.0 Seconds 6.6\n",
      "Step: 300 TLoss: 0.0 VLoss: 0.0 Seconds 6.5\n",
      "Step: 400 TLoss: 0.0 VLoss: 0.0 Seconds 6.3\n",
      "Step: 500 TLoss: 0.0 VLoss: 0.0 Seconds 6.2\n",
      "Step: 600 TLoss: 0.0 VLoss: 0.0 Seconds 6.2\n",
      "Step: 700 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 800 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 900 TLoss: 0.0 VLoss: 0.0 Seconds 6.2\n",
      "Step: 1000 TLoss: 0.0 VLoss: 0.0 Seconds 6.3\n",
      "Step: 1100 TLoss: 0.0 VLoss: 0.0 Seconds 6.3\n",
      "Step: 1200 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 1300 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 1400 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 1500 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 1600 TLoss: 0.0 VLoss: 0.0 Seconds 5.9\n",
      "Step: 1700 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 1800 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 1900 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 2000 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 2100 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 2200 TLoss: 0.0 VLoss: 0.0 Seconds 5.9\n",
      "Step: 2300 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 2400 TLoss: 0.0 VLoss: 0.0 Seconds 5.9\n",
      "Step: 2500 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 2600 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 2700 TLoss: 0.13097666 VLoss: 0.0 Seconds 5.9\n",
      "Step: 2800 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 2900 TLoss: 0.24215363 VLoss: 2.8876646 Seconds 6.0\n",
      "Step: 3000 TLoss: 0.1580896 VLoss: 3.1121829 Seconds 5.9\n",
      "Step: 3100 TLoss: 1.0246204 VLoss: 2.7034118 Seconds 6.0\n",
      "Step: 3200 TLoss: 0.47718078 VLoss: 4.055328 Seconds 5.9\n",
      "Step: 3300 TLoss: 2.993808 VLoss: 0.0 Seconds 6.0\n",
      "Step: 3400 TLoss: 0.7201544 VLoss: 0.0 Seconds 5.9\n",
      "Step: 3500 TLoss: 0.80998045 VLoss: 0.0 Seconds 6.0\n",
      "Step: 3600 TLoss: 0.7328143 VLoss: 0.0 Seconds 6.0\n",
      "Step: 3700 TLoss: 0.29549438 VLoss: 0.42169374 Seconds 5.9\n",
      "Step: 3800 TLoss: 0.0 VLoss: 2.2452047 Seconds 6.0\n",
      "Step: 3900 TLoss: 16.147575 VLoss: 0.0 Seconds 6.0\n",
      "Step: 4000 TLoss: 1.3331897 VLoss: 8.631939 Seconds 6.0\n",
      "Step: 4100 TLoss: 7.295362 VLoss: 0.0 Seconds 6.0\n",
      "Step: 4200 TLoss: 0.13148987 VLoss: 0.0 Seconds 6.0\n",
      "Step: 4300 TLoss: 0.85995543 VLoss: 0.0 Seconds 6.2\n",
      "Step: 4400 TLoss: 1.0712769 VLoss: 0.3888172 Seconds 5.9\n",
      "Step: 4500 TLoss: 0.24185427 VLoss: 0.0 Seconds 6.0\n",
      "Step: 4600 TLoss: 0.0 VLoss: 0.0 Seconds 5.9\n",
      "Step: 4700 TLoss: 4.237867 VLoss: 14.66931 Seconds 6.0\n",
      "Step: 4800 TLoss: 2.4429462 VLoss: 0.119135156 Seconds 6.0\n",
      "Step: 4900 TLoss: 3.9993246 VLoss: 3.3684344 Seconds 6.0\n",
      "Step: 5000 TLoss: 3.6562567 VLoss: 4.252177 Seconds 6.0\n",
      "Step: 5100 TLoss: 5.20821 VLoss: 4.1608953 Seconds 6.1\n",
      "Step: 5200 TLoss: 4.141115 VLoss: 0.2296953 Seconds 6.0\n",
      "Step: 5300 TLoss: 6.3626904 VLoss: 15.730094 Seconds 6.0\n",
      "Step: 5400 TLoss: 7.4083157 VLoss: 0.22972462 Seconds 6.0\n",
      "Step: 5500 TLoss: 1.5039731 VLoss: 0.22972462 Seconds 6.0\n",
      "Step: 5600 TLoss: 0.97107756 VLoss: 0.22972462 Seconds 6.0\n",
      "Step: 5700 TLoss: 0.019475708 VLoss: 0.22972462 Seconds 6.0\n",
      "Step: 5800 TLoss: 0.08939545 VLoss: 0.22972462 Seconds 6.0\n",
      "Step: 5900 TLoss: 0.9299951 VLoss: 0.0 Seconds 6.0\n",
      "Step: 6000 TLoss: 4.8652177 VLoss: 1.6200376 Seconds 5.9\n",
      "Step: 6100 TLoss: 1.2545843 VLoss: 1.5833883 Seconds 6.1\n",
      "Step: 6200 TLoss: 0.22429809 VLoss: 1.5900946 Seconds 5.9\n",
      "Step: 6300 TLoss: 0.0026947022 VLoss: 1.5833789 Seconds 6.0\n",
      "Step: 6400 TLoss: 0.41233826 VLoss: 0.17103282 Seconds 6.0\n",
      "Step: 6500 TLoss: 0.0 VLoss: 0.18753828 Seconds 6.0\n",
      "Step: 6600 TLoss: 0.9579483 VLoss: 0.23229766 Seconds 6.0\n",
      "Step: 6700 TLoss: 0.0 VLoss: 0.3041 Seconds 5.9\n",
      "Step: 6800 TLoss: 0.11878906 VLoss: 0.0 Seconds 6.0\n",
      "Step: 6900 TLoss: 0.7185647 VLoss: 0.0 Seconds 6.0\n",
      "Step: 7000 TLoss: 0.16499542 VLoss: 0.0 Seconds 6.0\n",
      "Step: 7100 TLoss: 2.1843717 VLoss: 1.4748281 Seconds 6.1\n",
      "Step: 7200 TLoss: 0.8494556 VLoss: 0.7360203 Seconds 6.2\n",
      "Step: 7300 TLoss: 3.0680041 VLoss: 0.0 Seconds 6.0\n",
      "Step: 7400 TLoss: 0.013930359 VLoss: 0.0 Seconds 6.0\n",
      "Step: 7500 TLoss: 1.0827398 VLoss: 0.0 Seconds 6.0\n",
      "Step: 7600 TLoss: 0.17607239 VLoss: 0.0 Seconds 6.0\n",
      "Step: 7700 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 7800 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 7900 TLoss: 0.34611452 VLoss: 0.0 Seconds 6.1\n",
      "Step: 8000 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 8100 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 8200 TLoss: 1.6118808 VLoss: 0.0 Seconds 6.0\n",
      "Step: 8300 TLoss: 0.23198456 VLoss: 0.0 Seconds 6.0\n",
      "Step: 8400 TLoss: 0.2083844 VLoss: 0.0 Seconds 6.0\n",
      "Step: 8500 TLoss: 1.9977609 VLoss: 0.13486601 Seconds 6.0\n",
      "Step: 8600 TLoss: 0.30722716 VLoss: 0.28851542 Seconds 6.1\n",
      "Step: 8700 TLoss: 3.0817704 VLoss: 2.818661 Seconds 6.0\n",
      "Step: 8800 TLoss: 1.4499396 VLoss: 4.245007 Seconds 6.0\n",
      "Step: 8900 TLoss: 0.0 VLoss: 4.246596 Seconds 6.0\n",
      "Step: 9000 TLoss: 0.0 VLoss: 4.246596 Seconds 6.0\n",
      "Step: 9100 TLoss: 0.22487213 VLoss: 4.1026783 Seconds 6.2\n",
      "Step: 9200 TLoss: 0.0 VLoss: 4.1026783 Seconds 6.0\n",
      "Step: 9300 TLoss: 0.0 VLoss: 4.1026783 Seconds 6.0\n",
      "Step: 9400 TLoss: 3.2582374 VLoss: 0.0 Seconds 6.0\n",
      "Step: 9500 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 9600 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 9700 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 9800 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 9900 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 10000 TLoss: 0.44910768 VLoss: 0.0 Seconds 6.0\n",
      "Step: 10100 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 10200 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 10300 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 10400 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 10500 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 10600 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 10700 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 10800 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 10900 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 11000 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 11100 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 11200 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 11300 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 11400 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 11500 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 11600 TLoss: 0.0 VLoss: 0.0 Seconds 5.9\n",
      "Step: 11700 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 11800 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 11900 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 12000 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 12100 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 12200 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 12300 TLoss: 0.0 VLoss: 0.0 Seconds 6.5\n",
      "Step: 12400 TLoss: 0.0 VLoss: 0.0 Seconds 6.8\n",
      "Step: 12500 TLoss: 0.0 VLoss: 0.0 Seconds 6.7\n",
      "Step: 12600 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 12700 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 12800 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 12900 TLoss: 0.0 VLoss: 0.0 Seconds 5.9\n",
      "Step: 13000 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 13100 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 13200 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 13300 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 13400 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 13500 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 13600 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 13700 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 13800 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 13900 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 14000 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 14100 TLoss: 0.0 VLoss: 0.0 Seconds 6.2\n",
      "Step: 14200 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 14300 TLoss: 0.0 VLoss: 0.0 Seconds 6.2\n",
      "Step: 14400 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 14500 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 14600 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 14700 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 14800 TLoss: 0.0 VLoss: 0.0 Seconds 6.3\n",
      "Step: 14900 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 15000 TLoss: 0.0 VLoss: 0.0 Seconds 6.2\n",
      "Step: 15100 TLoss: 0.0 VLoss: 0.0 Seconds 6.2\n",
      "Step: 15200 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 15300 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 15400 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 15500 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 15600 TLoss: 0.0 VLoss: 0.0 Seconds 6.0\n",
      "Step: 15700 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 15800 TLoss: 0.0 VLoss: 0.0 Seconds 6.2\n",
      "Step: 15900 TLoss: 0.0 VLoss: 0.0 Seconds 6.2\n",
      "Step: 16000 TLoss: 0.0 VLoss: 0.0 Seconds 6.4\n",
      "Step: 16100 TLoss: 0.0 VLoss: 0.0 Seconds 7.7\n",
      "Step: 16200 TLoss: 0.0 VLoss: 0.0 Seconds 7.3\n",
      "Step: 16300 TLoss: 0.0 VLoss: 0.0 Seconds 7.9\n",
      "Step: 16400 TLoss: 0.0 VLoss: 0.0 Seconds 6.4\n",
      "Step: 16500 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n",
      "Step: 16600 TLoss: 0.0 VLoss: 0.0 Seconds 6.3\n",
      "Step: 16700 TLoss: 0.0 VLoss: 0.0 Seconds 6.5\n",
      "Step: 16800 TLoss: 0.0 VLoss: 0.0 Seconds 6.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 16900 TLoss: 0.0 VLoss: 0.0 Seconds 6.8\n",
      "Step: 17000 TLoss: 0.0 VLoss: 0.0 Seconds 6.5\n",
      "Step: 17100 TLoss: 0.0 VLoss: 0.0 Seconds 7.0\n",
      "Step: 17200 TLoss: 0.0 VLoss: 0.0 Seconds 6.8\n",
      "Step: 17300 TLoss: 0.0 VLoss: 0.0 Seconds 6.5\n",
      "Step: 17400 TLoss: 0.0 VLoss: 0.0 Seconds 6.5\n",
      "Step: 17500 TLoss: 0.0 VLoss: 0.0 Seconds 7.2\n",
      "Step: 17600 TLoss: 0.0 VLoss: 0.0 Seconds 6.8\n",
      "Step: 17700 TLoss: 0.0 VLoss: 0.0 Seconds 6.5\n",
      "Step: 17800 TLoss: 0.0 VLoss: 0.0 Seconds 6.7\n",
      "Step: 17900 TLoss: 0.0 VLoss: 0.0 Seconds 6.8\n",
      "Step: 18000 TLoss: 0.0 VLoss: 0.0 Seconds 6.9\n",
      "Step: 18100 TLoss: 0.0 VLoss: 0.0 Seconds 7.0\n",
      "Step: 18200 TLoss: 0.0 VLoss: 0.0 Seconds 6.4\n",
      "Step: 18300 TLoss: 0.0 VLoss: 0.0 Seconds 6.9\n",
      "Step: 18400 TLoss: 0.0 VLoss: 0.0 Seconds 6.5\n",
      "Step: 18500 TLoss: 0.0 VLoss: 0.0 Seconds 6.2\n",
      "Step: 18600 TLoss: 0.0 VLoss: 0.0 Seconds 6.2\n",
      "Step: 18700 TLoss: 0.0 VLoss: 0.0 Seconds 6.7\n",
      "Step: 18800 TLoss: 11.075033 VLoss: 0.0 Seconds 6.6\n",
      "Step: 18900 TLoss: 7.2516384 VLoss: 3.1763575 Seconds 6.6\n",
      "Step: 19000 TLoss: 1.0144047 VLoss: 2.3311157 Seconds 6.6\n",
      "Step: 19100 TLoss: 2.3993235 VLoss: 3.5034406 Seconds 6.8\n",
      "Step: 19200 TLoss: 0.0 VLoss: 3.54645 Seconds 6.6\n",
      "Step: 19300 TLoss: 1.821184 VLoss: 1.2153344 Seconds 6.5\n",
      "Step: 19400 TLoss: 0.0 VLoss: 1.2153344 Seconds 6.9\n",
      "Step: 19500 TLoss: 0.23737 VLoss: 0.0 Seconds 6.5\n",
      "Step: 19600 TLoss: 0.0 VLoss: 0.0 Seconds 7.0\n",
      "Step: 19700 TLoss: 0.0 VLoss: 0.0 Seconds 6.6\n",
      "Step: 19800 TLoss: 0.0 VLoss: 0.0 Seconds 6.4\n",
      "Step: 19900 TLoss: 0.0 VLoss: 0.0 Seconds 7.0\n",
      "Step: 20000 TLoss: 0.0 VLoss: 0.0 Seconds 6.1\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "while step < 20000:\n",
    "    start = time.time()\n",
    "    \n",
    "    # shuffle train data once in while\n",
    "    if step % 1000 == 0:\n",
    "        data_train = data_train.sample(frac=1.)\n",
    "    # train part\n",
    "    train_loss = []\n",
    "    \n",
    "    for x, y in get_batches(\n",
    "        data_train['x'].values, data_train['x'].values, batch_size, n=100):\n",
    "        step += 1\n",
    "        _, batch_loss = sess.run([train_op, loss], {inputs: x, targets: y})\n",
    "        train_loss.append(batch_loss)\n",
    "            \n",
    "\n",
    "    # validation prat (one batch of \"validation_size\")\n",
    "    feed_dict = {inputs: data_valid['x'].values, targets: data_valid['x'].values}\n",
    "    valid_loss, x_vectors = sess.run([loss, embed], feed_dict)\n",
    "    y_vectors = sess.run(embed, {inputs: data_valid['x'].values})\n",
    "\n",
    "    # outputs\n",
    "    print('Step:', step, 'TLoss:', np.mean(train_loss), 'VLoss:', np.mean(valid_loss),\n",
    "          'Seconds %.1f' % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = sess.run(embeddings)\n",
    "vectors = pd.DataFrame(vectors, index=vocab.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4494"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = vocab.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.23115137, 'tried'),\n",
       " (0.23250142, 'statistic'),\n",
       " (0.23361155, 'firing'),\n",
       " (0.23450589, 'wearing'),\n",
       " (0.23545003, 'assume'),\n",
       " (0.23781925, 'controller'),\n",
       " (0.24547061, 'significantly'),\n",
       " (0.24866837, 'tax'),\n",
       " (0.25343916, 'matrix'),\n",
       " (0.25458717, 'habit'),\n",
       " (0.25981885, 'when'),\n",
       " (0.26332042, 'animated'),\n",
       " (0.26457113, 'blowing'),\n",
       " (0.2848406, 'kids'),\n",
       " (0.2853072, 'positive'),\n",
       " (0.28860277, 'glenn'),\n",
       " (0.2981456, 'fractals'),\n",
       " (0.40886068, 'detroit'),\n",
       " (0.42984682, 'glenns'),\n",
       " (1.0, 'then')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "test_word = \"then\"\n",
    "\n",
    "similarities = []\n",
    "for word in word_list:\n",
    "    item = (cosine_similarity(vectors.loc[[test_word]], vectors.loc[[word]])[0][0], word)\n",
    "    similarities.append(item)\n",
    "similarities = sorted(similarities)\n",
    "similarities[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.24393196, 'use'),\n",
       " (0.24490309, 'microwaves'),\n",
       " (0.24665128, 'photographs'),\n",
       " (0.24775606, 'fewer'),\n",
       " (0.2480158, 'specification'),\n",
       " (0.25069916, 'ascii'),\n",
       " (0.25761282, 'bound'),\n",
       " (0.26392493, 'attached'),\n",
       " (0.2674393, 'al'),\n",
       " (0.27037475, 'additionally'),\n",
       " (0.27110022, 'rle'),\n",
       " (0.27259976, 'branch'),\n",
       " (0.27281642, 'mine'),\n",
       " (0.27500162, 'religion'),\n",
       " (0.2776731, 'tromsoe'),\n",
       " (0.28250164, 'seems'),\n",
       " (0.28782457, 'random'),\n",
       " (0.2947551, 'lift'),\n",
       " (0.3184655, 'pull'),\n",
       " (0.9999999, 'four')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "test_word = \"four\"\n",
    "\n",
    "similarities = []\n",
    "for word in word_list:\n",
    "    item = (cosine_similarity(vectors.loc[[test_word]], vectors.loc[[word]])[0][0], word)\n",
    "    similarities.append(item)\n",
    "similarities = sorted(similarities)\n",
    "similarities[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-b28f53f9ed72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m       perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n\u001b[1;32m     24\u001b[0m   \u001b[0mplot_only\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mlow_dim_embs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;31m#   labels = [reverse_dictionary[i] for i in xrange(plot_only)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mplot_with_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow_dim_embs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tsne.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m         \"\"\"\n\u001b[0;32m--> 858\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    768\u001b[0m                           \u001b[0mX_embedded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_embedded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m                           \u001b[0mneighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneighbors_nn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m                           skip_num_points=skip_num_points)\n\u001b[0m\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_tsne\u001b[0;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0mP\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_exaggeration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         params, kl_divergence, it = _gradient_descent(obj_func, params,\n\u001b[0;32m--> 812\u001b[0;31m                                                       **opt_args)\n\u001b[0m\u001b[1;32m    813\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m             print(\"[t-SNE] KL divergence after %d iterations with early \"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_gradient_descent\u001b[0;34m(objective, p0, it, n_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mgrad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_kl_divergence\u001b[0;34m(params, P, degrees_of_freedom, n_samples, n_components, skip_num_points)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0mdist\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mdist\u001b[0m \u001b[0;34m**=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdegrees_of_freedom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMACHINE_EPSILON\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;31m# Optimization trick below: np.dot(x, y) is faster than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "def plot_with_labels(low_dim_embs, labels, filename):\n",
    "  assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "  plt.figure(figsize=(18, 18))  # in inches\n",
    "  for i, label in enumerate(labels):\n",
    "    x, y = low_dim_embs[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        xy=(x, y),\n",
    "        xytext=(5, 2),\n",
    "        textcoords='offset points',\n",
    "        ha='right',\n",
    "        va='bottom')\n",
    "\n",
    "  plt.savefig(filename)\n",
    "\n",
    "try:\n",
    "  from sklearn.manifold import TSNE\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  tsne = TSNE(\n",
    "      perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "  plot_only = 500\n",
    "  low_dim_embs = tsne.fit_transform(vectors)\n",
    "#   labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "  plot_with_labels(low_dim_embs, word_list, os.path.join('tsne.png'))\n",
    "\n",
    "except ImportError as ex:\n",
    "  print('Please install sklearn, matplotlib, and scipy to show embeddings.')\n",
    "  print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
