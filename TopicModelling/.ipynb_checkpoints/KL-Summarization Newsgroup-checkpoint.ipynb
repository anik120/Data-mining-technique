{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import nltk\n",
    "import math\n",
    "import sys\n",
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(['http://localhost:9200/'])\n",
    "doc = {\n",
    "        'size' : 301,\n",
    "        'query': {\n",
    "            'match_all' : {}\n",
    "       }\n",
    "   }\n",
    "res = es.search(index='duc', doc_type='document', body=doc,scroll='1m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accept_sentence(sentence_divergent_dict):\n",
    "    max_divergence = 0\n",
    "    ret_sentence = \"\"\n",
    "    for sentence in sentence_divergent_dict:\n",
    "        if sentence_divergent_dict[sentence] > max_divergence:\n",
    "            max_divergence = sentence_divergent_dict[sentence]\n",
    "            ret_sentence = sentence\n",
    "    return ret_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = {}\n",
    "for item in res['hits']['hits']:\n",
    "    text = item['_source']['doc_text']\n",
    "    id_ = item['_source']['doc_id']\n",
    "    sentences = text.split(\".\")\n",
    "    text = nltk.tokenize.word_tokenize(text)\n",
    "    for word in text:\n",
    "        if not d.check(word):\n",
    "            if len(word) < 4:\n",
    "                text.remove(word)\n",
    "    global_dist = nltk.FreqDist(text)\n",
    "    i = 0\n",
    "    summary = \"\"\n",
    "    while i < 10 and len(sentences) != 0:\n",
    "        sentence_divergence_dict = {}\n",
    "        for sentence in sentences:\n",
    "            merged_text = nltk.tokenize.word_tokenize(summary + sentence)\n",
    "            merged_dist = nltk.FreqDist(merged_text)\n",
    "            divergence = 0\n",
    "            for word in global_dist:\n",
    "                if word in merged_dist:\n",
    "                    if merged_dist[word] != 0:\n",
    "                        divergence += global_dist[word] * math.log(global_dist[word] / merged_dist[word])\n",
    "            sentence_divergence_dict[sentence] = divergence\n",
    "        sentence_to_add = accept_sentence(sentence_divergence_dict)\n",
    "        summary += sentence_to_add + \". \"\n",
    "        if sentence_to_add in sentences:\n",
    "            sentences.remove(sentence_to_add)\n",
    "        i += 1\n",
    "    summaries[id_] = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_in_elastic = {}\n",
    "for item in res['hits']['hits']:\n",
    "    id_ = item['_source']['doc_id']\n",
    "    gold_in_elastic[id_] = item['_source']['gold_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_in_elastic = {}\n",
    "for item in res['hits']['hits']:\n",
    "    id_ = item['_source']['doc_id']\n",
    "    docs_in_elastic[id_] = item['_source']['doc_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'index': 'duc', 'shards_acknowledged': True}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.delete(index='duc', ignore=[400, 404])\n",
    "mappings_duc = {\n",
    "    'mappings':{\n",
    "        'document':{\n",
    "            'properties':{\n",
    "                'doc_id': {'type': 'text', 'index': 'false'},\n",
    "                'doc_text': {'type': 'text', 'analyzer': 'english'},\n",
    "                'gold_summary':{'type': 'text', 'analyzer': 'english'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "es.indices.create(index=\"duc\", body=mappings_duc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for id_ in docs_in_elastic:\n",
    "    docs.append({\n",
    "        'doc_id' : id_,\n",
    "        'doc_text': docs_in_elastic[id_],\n",
    "        'gold_summary': gold_in_elastic[id_],\n",
    "        'lda_summary' : summaries[id_]\n",
    "    })\n",
    "for doc in docs:\n",
    "    es.index(index='duc', doc_type='document', body=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
