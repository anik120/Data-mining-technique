{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import nltk\n",
    "import math\n",
    "import sys\n",
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(['http://localhost:9200/'])\n",
    "doc = {\n",
    "        'size' : 10000,\n",
    "        'query': {\n",
    "            'match_all' : {}\n",
    "       }\n",
    "   }\n",
    "res = es.search(index='newsgroup', doc_type='document', body=doc,scroll='1m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accept_sentence(sentence_divergent_dict):\n",
    "    max_divergence = 0\n",
    "    ret_sentence = \"\"\n",
    "    for sentence in sentence_divergent_dict:\n",
    "        if sentence_divergent_dict[sentence] > max_divergence:\n",
    "            max_divergence = sentence_divergent_dict[sentence]\n",
    "            ret_sentence = sentence\n",
    "    return ret_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = {}\n",
    "for item in res['hits']['hits']:\n",
    "    text = item['_source']['doc_text']\n",
    "    id_ = item['_source']['doc_id']\n",
    "    sentences = text.split(\".\")\n",
    "    text = nltk.tokenize.word_tokenize(text)\n",
    "    for word in text:\n",
    "        if not d.check(word):\n",
    "            if len(word) < 4:\n",
    "                text.remove(word)\n",
    "    global_dist = nltk.FreqDist(text)\n",
    "    i = 0\n",
    "    summary = \"\"\n",
    "    while i < 10 and len(sentences) != 0:\n",
    "        sentence_divergence_dict = {}\n",
    "        for sentence in sentences:\n",
    "            merged_text = nltk.tokenize.word_tokenize(summary + sentence)\n",
    "            merged_dist = nltk.FreqDist(merged_text)\n",
    "            divergence = 0\n",
    "            for word in global_dist:\n",
    "                if word in merged_dist:\n",
    "                    if merged_dist[word] != 0:\n",
    "                        divergence += global_dist[word] * math.log(global_dist[word] / merged_dist[word])\n",
    "            sentence_divergence_dict[sentence] = divergence\n",
    "        sentence_to_add = accept_sentence(sentence_divergence_dict)\n",
    "        summary += sentence_to_add + \". \"\n",
    "        if sentence_to_add in sentences:\n",
    "            sentences.remove(sentence_to_add)\n",
    "        i += 1\n",
    "    summaries[id_] = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' So why search a plane for weapons\\t        since it\\'s content is announced to be weapons?   \\tIf there is one that\\'s confused then that\\'s you! We have the right (and we do)\\tto give weapons to the Azeris, since Armenians started the fight in Azerbadjan! |>You are correct, all Turkish planes should be simply shot down! Nice, slow|>moving air transports!\\tShoot down with what? Armenian bread and butter? Or the arms and personel \\tof the Russian army?. \\tThe area will be \"greater\" after some years, like your \"holocaust\" numbers. \\ti        Turkey\\'s government has announced that it\\'s giving weapons  <-----------i        to Azerbadjan since Armenia started to attack Azerbadjan\\t\\t        it self, not the Karabag province.  \\t\\t\\t    ***************\\tNOTHING OF THE MENTIONED IS TRUE, BUT LET SAY IT\\'s TRUE. . . . . . . '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries['3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_in_elastic = {}\n",
    "for item in res['hits']['hits']:\n",
    "    id_ = item['_source']['doc_id']\n",
    "    docs_in_elastic[id_] = item['_source']['doc_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'index': 'newsgroup', 'shards_acknowledged': True}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.delete(index='newsgroup', ignore=[400, 404])\n",
    "mappings_duc = {\n",
    "    'mappings':{\n",
    "        'document':{\n",
    "            'properties':{\n",
    "                'doc_id': {'type': 'text', 'index': 'false'},\n",
    "                'doc_text': {'type': 'text', 'analyzer': 'english'},\n",
    "                'kl_summary':{'type': 'text', 'analyzer': 'english'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "es.indices.create(index=\"newsgroup\", body=mappings_duc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for id_ in docs_in_elastic:\n",
    "    docs.append({\n",
    "        'doc_id' : id_,\n",
    "        'doc_text': docs_in_elastic[id_],\n",
    "        'kl_summary' : summaries[id_]\n",
    "    })\n",
    "for doc in docs:\n",
    "    es.index(index='newsgroup', doc_type='document', body=doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
