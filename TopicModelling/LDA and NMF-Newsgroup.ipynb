{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_samples = 2000\n",
    "# n_features = 1000\n",
    "n_components = 20\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch(['http://localhost:9200/'])\n",
    "doc = {\n",
    "        'size' : 10000,\n",
    "        'query': {\n",
    "            'match_all' : {}\n",
    "       }\n",
    "   }\n",
    "res = es.search(index='newsgroup', doc_type='document', body=doc,scroll='1m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for item in res['hits']['hits']:\n",
    "    docs.append(item['_source']['doc_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        topic_words_dict[topic_idx] = message.split()[2:] \n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "done in 4.503s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(docs)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 3.985s.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(docs)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model with tf-idf features\n",
      "done in 11.082s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting the NMF model with tf-idf features\")\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "Topic #0: don people just com like think know time good writes ve want right make article way does new really use\n",
      "Topic #1: file files program ftp image format graphics gif images data software use available server color pc directory pub display package\n",
      "Topic #2: god jesus bible faith believe christ christian christians sin heaven church belief religion man say life truth rutgers christianity lord\n",
      "Topic #3: drive disk hard drives floppy ide boot bios hd controller cd problem meg slave seagate rom internal pin disks mb\n",
      "Topic #4: chip clipper encryption government phone law enforcement wiretap privacy algorithm security escrow crypto secure nsa phones technology encrypted administration use\n",
      "Topic #5: edu article writes cs uiuc cso cc news com andrew cmu au rutgers ohio cwru david 1993 state umd acs\n",
      "Topic #6: windows dos os microsoft nt ms mouse run version running apps using driver problem pc use mode memory drivers font\n",
      "Topic #7: game games team hockey espn baseball year players play season fans win league series player nhl teams devils pens leafs\n",
      "Topic #8: key keys escrow bit des s1 bits serial s2 80 pgp public random security secret number algorithm session encrypted court\n",
      "Topic #9: card monitor video vga cards bus ram drivers mac bit ati 16 pc motherboard mode memory svga board isa graphics\n",
      "Topic #10: israel israeli jews arab arabs jewish peace lebanese israelis palestinians palestinian palestine war policy igc apc soldiers cpr land state\n",
      "Topic #11: car cars dealer engine ford insurance clutch tires new miles oil price mustang speed saturn 000 accident shifter looking mileage\n",
      "Topic #12: thanks advance mail does know hi info looking address help email appreciated anybody appreciate information hello send interested reply like\n",
      "Topic #13: fbi koresh batf children compound sandvik gas davidians agents roby bd cult started tear kent warrant government udel did waco\n",
      "Topic #14: window manager application xterm motif expose problem widget use event user using screen button server decoration colormap olwm cursor position\n",
      "Topic #15: cramer optilink clayton homosexual men gay homosexuals sexual partners com sex promiscuous male homosexuality study number bi percent heterosexual median\n",
      "Topic #16: scsi ide controller bus devices isa pc data mac drives dma adaptec interface transfer faster nmsu oracle sec esdi mb\n",
      "Topic #17: objective morality moral livesey values frank sgi jon solntze wpd keith dwyer d012s658 cobb caltech natural immoral subjective horus mchp\n",
      "Topic #18: space nasa shuttle launch gov henry alaska station aurora orbit moon jsc mission earth cost hst nsmca zoo jpl lunar\n",
      "Topic #19: bike bikes ride riding motorcycle honda miles advice buy rider buying dod riders wave harley good insurance com helmet going\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features\n",
      "done in 74.009s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features\")\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
      "Topic #0: just writes think like time way don know want ve really right people sure thing things make say good article\n",
      "Topic #1: windows use thanks using need know software like version does work program want drive help used pc ve run dos\n",
      "Topic #2: god rutgers jesus say believe bible christian christians 1993 religion people true christ word church faith man christianity truth does\n",
      "Topic #3: year game writes team play edu games season time players night think hockey good teams win player series red years\n",
      "Topic #4: government use key law people used security state using number public keys clipper secure order make private phone chip encryption\n",
      "Topic #5: writes edu article com wrote david state robert cs news use uucp says michael cc netcom steve pitt usenet mean\n",
      "Topic #6: war people jews israel state world policy rights years jewish killing killed israeli fact peace political states population government muslim\n",
      "Topic #7: writes com think article team win netcom steve year say runs run game won dave start ca let better net\n",
      "Topic #8: writes edu article university use cs public michael used news subject read source sorry phone stanford mit line working program\n",
      "Topic #9: card video monitor mac ram work vga support se speed cards simms sale memory modem price does serial bit motherboard\n",
      "Topic #10: 1993 following 10 20 apr 93 19 25 15 16 18 27 13 22 subject 30 24 11 21 date\n",
      "Topic #11: new used sale car power use price good old test year engine condition years great miles model sell low speed\n",
      "Topic #12: thanks mail know looking post does new interested email info send thank reply advance hi list wondering information tell hello\n",
      "Topic #13: people gun koresh fbi waco did weapons today news started said government edu guns shot wanted think children texas gas\n",
      "Topic #14: window problem using use set x11r5 thanks work sun server running file tried program line motif try xterm start mit\n",
      "Topic #15: men women people edu study sex wife sexual homosexuals gay male homosexual optilink school number shows posted cramer man don\n",
      "Topic #16: writes does speed uk article performance rate faster heard car bus ca ve looking information report fast data work gov\n",
      "Topic #17: make uk good offer sgi point value people simple sell jon trade free don uucp looking want wrong questions view\n",
      "Topic #18: space writes nasa article orbit science gov uiuc earth shuttle station need launch sun research moon news sky mission uxa\n",
      "Topic #19: writes ve bike ca article like ride just wrote don riding road motorcycle dod work time got bikes says wave\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features...\n",
      "done in 61.442s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting LDA models with tf features...\")\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: god jesus jews church christ armenian bible jewish armenians christian lord turkish muslim father muslims war faith jehovah history greek\n",
      "Topic #1: printer laser print hp homeopaths printers page homeopathy bernadette paper lady deskjet toner ink canon cartridges lsd laserjet fyi promo\n",
      "Topic #2: edu fbi writes article koresh batf compound gas pitt davidians cult bd tear purdue inside agents banks started tank audio\n",
      "Topic #3: windows use file dos program edu software available data window using information ftp files mail server version graphics image com\n",
      "Topic #4: sex homosexual men homosexuality gay com cramer writes sexual article optilink homosexuals clayton marriage michael number key male paul christians\n",
      "Topic #5: edu writes article drive game like just good year know ca don team card think does new games space time\n",
      "Topic #6: 25 10 00 12 11 15 20 16 17 14 1993 18 13 30 28 40 24 21 27 50\n",
      "Topic #7: modem port serial battery card apple radar lc ports adapter com mac amp irq use nubus cable detector cb phone\n",
      "Topic #8: mr stephanopoulos jur space president sky light henry riders uic vega uicvm dee earth carleton kratz billboard cellar spencer ir\n",
      "Topic #9: pom diana xxxxx stefan jpw sunset daylight ohm spice ashok cablevision gusto rosie cinema knx drexel tilt tci utxvms tracy\n",
      "Topic #10: writes edu people article don com just like know think time does way say right good make said did ve\n",
      "Topic #11: ax max 145 b8f 2di g9v bhj 75u 0d a86 pl bh 1d9 1t ah giz air 3t oj ao\n",
      "Topic #12: players baseball hits defensive gant average year career dale duke hirschbeck guys dave strike hit infield solder boggs wade ryan\n",
      "Topic #13: medical health msg doctor vitamin disease patients treatment food cancer drugs use symptoms drug blood candida patient insurance hiv body\n",
      "Topic #14: com gatech nick writes article nixon prism ingres behanna yugoslavia garrett vt senate dod agrep gt yale hydra syl secession\n",
      "Topic #15: thou thy xv es dance velocity define john defined mpr echo endif speedy gardner protection earth shall g3states copy carpet\n",
      "Topic #16: captain traded marc istanbul kk expose ankara resigned ermeni azerbaijan karabag agdam trivia azeri osmanli turkey hojali striped robitaille collingridge\n",
      "Topic #17: apartment sumgait azerbaijani 1st mamma azerbaijanis copies cover appears vs comics man baku art karina rider hulk balcony shouting wolverine\n",
      "Topic #18: q45 engine gm engines launch station tank diesel orbit john v6 dk shaft launching msus mercedes providence sg rtsg rls\n",
      "Topic #19: w7 lk ah cx t7 uw t5 a7 te ck k8 chz a4 hz 7u w1 mc 2q kh sp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "tf_feature_probs = tf_vectorizer.max_features\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features...\n",
      "done in 62.839s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting LDA models with tf features...\")\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit_transform(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: god jesus jews church christ armenian bible jewish armenians christian lord turkish muslim father muslims war faith jehovah history greek\n",
      "Topic #1: printer laser print hp homeopaths printers page homeopathy bernadette paper lady deskjet toner ink canon cartridges lsd laserjet fyi promo\n",
      "Topic #2: edu fbi writes article koresh batf compound gas pitt davidians cult bd tear purdue inside agents banks started tank audio\n",
      "Topic #3: windows use file dos program edu software available data window using information ftp files mail server version graphics image com\n",
      "Topic #4: sex homosexual men homosexuality gay com cramer writes sexual article optilink homosexuals clayton marriage michael number key male paul christians\n",
      "Topic #5: edu writes article drive game like just good year know ca don team card think does new games space time\n",
      "Topic #6: 25 10 00 12 11 15 20 16 17 14 1993 18 13 30 28 40 24 21 27 50\n",
      "Topic #7: modem port serial battery card apple radar lc ports adapter com mac amp irq use nubus cable detector cb phone\n",
      "Topic #8: mr stephanopoulos jur space president sky light henry riders uic vega uicvm dee earth carleton kratz billboard cellar spencer ir\n",
      "Topic #9: pom diana xxxxx stefan jpw sunset daylight ohm spice ashok cablevision gusto rosie cinema knx drexel tilt tci utxvms tracy\n",
      "Topic #10: writes edu people article don com just like know think time does way say right good make said did ve\n",
      "Topic #11: ax max 145 b8f 2di g9v bhj 75u 0d a86 pl bh 1d9 1t ah giz air 3t oj ao\n",
      "Topic #12: players baseball hits defensive gant average year career dale duke hirschbeck guys dave strike hit infield solder boggs wade ryan\n",
      "Topic #13: medical health msg doctor vitamin disease patients treatment food cancer drugs use symptoms drug blood candida patient insurance hiv body\n",
      "Topic #14: com gatech nick writes article nixon prism ingres behanna yugoslavia garrett vt senate dod agrep gt yale hydra syl secession\n",
      "Topic #15: thou thy xv es dance velocity define john defined mpr echo endif speedy gardner protection earth shall g3states copy carpet\n",
      "Topic #16: captain traded marc istanbul kk expose ankara resigned ermeni azerbaijan karabag agdam trivia azeri osmanli turkey hojali striped robitaille collingridge\n",
      "Topic #17: apartment sumgait azerbaijani 1st mamma azerbaijanis copies cover appears vs comics man baku art karina rider hulk balcony shouting wolverine\n",
      "Topic #18: q45 engine gm engines launch station tank diesel orbit john v6 dk shaft launching msus mercedes providence sg rtsg rls\n",
      "Topic #19: w7 lk ah cx t7 uw t5 a7 te ck k8 chz a4 hz 7u w1 mc 2q kh sp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words_prob_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  0 :  [0.022353863512304704, 0.01857223688734366, 0.013142050477154306, 0.010164428399121406, 0.009649175237921189, 0.009481590608859402, 0.009182733881668197, 0.008810619273694325, 0.008252406188337906, 0.008071670724199356, 0.007747367529330197, 0.006530142785831521, 0.00598616983843073, 0.005964128079852212, 0.0057266400698403885, 0.005665463767801227, 0.005485822640806479, 0.005339510626071642, 0.004961413131466832, 0.004599881602261312]\n",
      "Topic  1 :  [0.03236852947575337, 0.021288242805157506, 0.01807804880086793, 0.012451241723073578, 0.012067933764167522, 0.008971211430372158, 0.008264992461792662, 0.008079728702301845, 0.007961202671014796, 0.006943065801416202, 0.006798851762284424, 0.005711288147324003, 0.005547253377406439, 0.005503620984488836, 0.005326777834433815, 0.004360142646090509, 0.0040528598678197724, 0.0036398809155446066, 0.0035332036871165593, 0.0034419750690575864]\n",
      "Topic  2 :  [0.03839751234427181, 0.02602336441151551, 0.01921959019789326, 0.018947718982277588, 0.018617673908773478, 0.016400577633662382, 0.014694023248050563, 0.007833079673725969, 0.006897680340470456, 0.006705052430716626, 0.006471778537013385, 0.006432873715001876, 0.005700241412108546, 0.005533902956448688, 0.00553093585383975, 0.005418692858735559, 0.005091663351266294, 0.005079883745894581, 0.0050766262488534655, 0.004764014512132838]\n",
      "Topic  3 :  [0.00945630440008311, 0.008267000904045555, 0.007772828962312282, 0.007143960887744319, 0.006509021132045457, 0.00597555385324181, 0.005941351332334004, 0.005610367699252165, 0.005420072867608257, 0.005361615397664333, 0.005195202706723522, 0.004832278220484561, 0.004825659430631772, 0.0047167644717625535, 0.004491010398463789, 0.0044250121103385906, 0.004343086191644157, 0.004123905104562479, 0.003803437006940034, 0.003629951098824502]\n",
      "Topic  4 :  [0.015226109574247963, 0.01323274615353247, 0.012328493206298814, 0.01203663466803019, 0.011956803885003408, 0.010054381659768517, 0.009695590886984701, 0.00932246214555893, 0.009124740342147369, 0.00877556185949191, 0.008130792173880944, 0.008118793619864777, 0.006915841781435879, 0.006904384489299839, 0.006159224061296293, 0.006143786395417327, 0.005759432671743015, 0.005682487047196761, 0.005473043960859527, 0.004756276630069319]\n",
      "Topic  5 :  [0.01038507220640996, 0.007368638433159006, 0.00592010162019603, 0.005787999709417233, 0.005152489874543142, 0.005073338268578788, 0.004810514708473698, 0.004433919029877755, 0.004308750481854685, 0.004284612151083682, 0.004280189498958385, 0.004008804472283483, 0.003964371457306848, 0.003731890521051276, 0.003699311549113605, 0.003617357615575335, 0.0033452442544940368, 0.0031220133003160466, 0.003079382430651915, 0.0029373210566262247]\n",
      "Topic  6 :  [0.01580680516758993, 0.014711155812936548, 0.009857052083921035, 0.009331606069762301, 0.009104053083749132, 0.00884937168463375, 0.00786884065431465, 0.007582090790930226, 0.007323600505437585, 0.007182605752265268, 0.00645568819813573, 0.0063789688411761475, 0.00615355729885533, 0.006068185664894975, 0.005962020296308397, 0.005855411894125526, 0.005844720240749167, 0.005559804416454607, 0.005469484720712184, 0.005453670537313971]\n",
      "Topic  7 :  [0.017915365730813815, 0.017709627205810218, 0.016440489588074522, 0.012989368591086965, 0.01148437304398508, 0.009318126731199089, 0.008953662455146812, 0.008658395127706693, 0.00850458522265415, 0.008321662193822235, 0.008048871741624618, 0.00770771607272551, 0.007348711831023502, 0.007228245618470201, 0.0071563173814958286, 0.006711042277607742, 0.006545849547671132, 0.006441555049189028, 0.006057251640343495, 0.00602976611613975]\n",
      "Topic  8 :  [0.04718875754886299, 0.0461187150917345, 0.03578074549437485, 0.020888262017751773, 0.01916542302521952, 0.015076385250646626, 0.014531826460656287, 0.008330883322141582, 0.006477750277388792, 0.0055186337842520515, 0.004976409509702241, 0.0048770777593891024, 0.004844039656801442, 0.00450140359147189, 0.004362006599623733, 0.0039602455361102745, 0.0038534684833829145, 0.0038423015058099232, 0.003524412097699596, 0.0033875426700305544]\n",
      "Topic  9 :  [0.012402797237660456, 0.008656199034747495, 0.006062493750671285, 0.005454415141825036, 0.005034105988936792, 0.005028784924271049, 0.004626792475842512, 0.004308486841668841, 0.004131639437423601, 0.004119757645254376, 0.003616980779282902, 0.003455167778510653, 0.002793826494893276, 0.002767345268930013, 0.002621486048956375, 0.0025168185697794646, 0.0024078181418629475, 0.002279116690742436, 0.002258057407150876, 0.0022047767803772076]\n",
      "Topic  10 :  [0.008143681481828162, 0.007867722110848208, 0.007148649396149781, 0.006903894160594755, 0.006230678871493272, 0.005686718684832834, 0.005661115639205487, 0.005084639138544457, 0.004916471533707443, 0.004795485326621287, 0.0038761052620576047, 0.003393432565740956, 0.00322192129526947, 0.003111470874684739, 0.0030612966813561788, 0.0029128513031451172, 0.0029116292143790503, 0.002736352810640007, 0.0027301656400058254, 0.0026211963315129153]\n",
      "Topic  11 :  [0.5179220846453955, 0.037000394771134454, 0.01054446529610709, 0.0102728883638196, 0.00979942286395854, 0.009705641202404528, 0.006649622689192584, 0.006486181950618185, 0.0064617905383323165, 0.0063264286571867645, 0.005685771523957376, 0.0042775878009940075, 0.004186034044162464, 0.003974977973943453, 0.003407417140192753, 0.003292031331703893, 0.0028171368051883097, 0.002702229630715854, 0.00269242393172473, 0.0025698998867281636]\n",
      "Topic  12 :  [0.011950554729053462, 0.011187492847871613, 0.010809362948909228, 0.009207640175917913, 0.00910980825811908, 0.008786782330865845, 0.008606946618703886, 0.006570335419410493, 0.006517634366724331, 0.006157649315387386, 0.00577691847365114, 0.005378323267605994, 0.005126948570695243, 0.004926335898845738, 0.0044910272744856615, 0.004468939478199548, 0.004455786927705634, 0.004413061667723046, 0.00436209942120296, 0.004170826500831074]\n",
      "Topic  13 :  [0.010587688235289642, 0.009730621962013642, 0.007652765568724798, 0.007600106523929469, 0.0073787201982117535, 0.007352672728763925, 0.007076774578066341, 0.006910160731811785, 0.006637563346888388, 0.006569449552296467, 0.005891176030167162, 0.005576069400042277, 0.0053724293423190895, 0.005042683426695989, 0.004986338779704073, 0.004911489000366991, 0.00486781425652289, 0.004641148971740428, 0.004640913993635961, 0.004631653751566129]\n",
      "Topic  14 :  [0.019313230959347647, 0.014094639711201402, 0.011435396048585173, 0.007307856551774964, 0.006687946561133473, 0.006444995500810479, 0.0063431431942749415, 0.006072581560312941, 0.005937361237739527, 0.005715345921814717, 0.005273702651228335, 0.004187676778540838, 0.004109108122366925, 0.004098737588924615, 0.004039502088003526, 0.003979315614823127, 0.003875104823506947, 0.0037721595122819445, 0.0034777419847435887, 0.0034360822923471678]\n",
      "Topic  15 :  [0.014086836804660935, 0.009340726460873278, 0.006744630289711902, 0.006729300277316995, 0.005718462710296667, 0.005689366551992438, 0.0047719426225281735, 0.004648557585880602, 0.004443601461939288, 0.004374873885751668, 0.003902484639161108, 0.003832811308590144, 0.0038153616965295118, 0.0037335092936788927, 0.003526565572252912, 0.0035024683038112985, 0.003439663421262006, 0.0033082946255065518, 0.003106797514879289, 0.002955860351106435]\n",
      "Topic  16 :  [0.021899879746841832, 0.011972120079823978, 0.01101796300120858, 0.007999950828292877, 0.00718631718891028, 0.005807745496096398, 0.0052563459125458505, 0.004271130717831009, 0.0039557617555887065, 0.003827640650233131, 0.0036839148978384164, 0.0036290240752248066, 0.0035748742171221263, 0.003555628671111193, 0.003439692494520734, 0.0033229115033354224, 0.003291795649159091, 0.0032743644258061083, 0.0030637364900390402, 0.002827687112680661]\n",
      "Topic  17 :  [0.015360813275608539, 0.014087048587590904, 0.012496428766343012, 0.012050275968666237, 0.010491064966360184, 0.009667146134382232, 0.009382484406370508, 0.00888124605040249, 0.008660899968603256, 0.008447597148275936, 0.008374430285103955, 0.007903391992823021, 0.007863789786708804, 0.007120826507121338, 0.006477767101512655, 0.006138074336491909, 0.006060548666931085, 0.005765248428306949, 0.005374268796057428, 0.0050840993860037965]\n",
      "Topic  18 :  [0.05597062173664502, 0.014834899792093301, 0.013386734022665978, 0.011049753447827926, 0.009600954087631348, 0.006647716373797956, 0.0057711300341821885, 0.005094584812502642, 0.004848127586264329, 0.004245391408619219, 0.00416615888224624, 0.004151566489248846, 0.003981855875949147, 0.003587635552419588, 0.0034623148243852347, 0.003369910914389901, 0.0033604824671900203, 0.0032233059533034276, 0.0032033134230285776, 0.003201601156803645]\n",
      "Topic  19 :  [0.010400133174166187, 0.007821783536171313, 0.007780528727785735, 0.007206896953402821, 0.006763883217547552, 0.00658616819080474, 0.005761711486132818, 0.005696023312336272, 0.005243486110030675, 0.005203044255401526, 0.004975510517907304, 0.004774624581207479, 0.004665012880587925, 0.0042009323345340785, 0.00412344000112682, 0.0041084530617016375, 0.0041048831206547355, 0.004048480154290824, 0.0040206759941542195, 0.0038638140010798113]\n"
     ]
    }
   ],
   "source": [
    "lda.components_ /= lda.components_.sum(axis=1)[:, np.newaxis]\n",
    "for i in range(20):\n",
    "    topic_words_prob_dict[i] = sorted(lda.components_[i])[::-1][:20] \n",
    "    print(\"Topic \",i,\": \", sorted(lda.components_[i])[::-1][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")\n",
    "def predict_doc_topic(doc, topic_words_dict, topic_words_prob_dict):\n",
    "    text = nltk.tokenize.word_tokenize(doc)\n",
    "    for word in text:\n",
    "        if not d.check(word):\n",
    "            if len(word) < 4:\n",
    "                text.remove(word)\n",
    "    temp_dist = nltk.FreqDist(text)\n",
    "    word_dist = {}\n",
    "    for word in temp_dist:\n",
    "        word_dist[word] = temp_dist[word]\n",
    "    for word in temp_dist:\n",
    "        if word_dist[word] > 10:\n",
    "            word_dist.pop(word)\n",
    "    doc_topics_prob = {}\n",
    "    for topic in topic_words_dict:\n",
    "        score = 0\n",
    "        word_list = topic_words_dict[topic]\n",
    "        prob_list = topic_words_prob_dict[topic]\n",
    "        for index in range(len(word_list)):\n",
    "            if word_list[index] in word_dist:\n",
    "                score += word_dist[word_list[index]] * prob_list[index]\n",
    "        doc_topics_prob[topic] = score\n",
    "    return doc_topics_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "docs = []\n",
    "for item in res['hits']['hits']:\n",
    "    id_ = item['_source']['doc_id']\n",
    "    doc = item['_source']['doc_text']\n",
    "    kl_summary = item['_source']['kl_summary']\n",
    "    doc_topics = predict_doc_topic(doc, topic_words_dict, topic_words_prob_dict)\n",
    "    sorted_dict = sorted(doc_topics.items(), key=operator.itemgetter(1))\n",
    "    doc_topics = sorted_dict[::-1][:5]\n",
    "    docs.append({\n",
    "        'doc_id' : id_,\n",
    "        'doc_text': doc,\n",
    "        'kl_summary' : kl_summary,\n",
    "        'doc_topics' : doc_topics\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_id': '17283',\n",
       " 'doc_text': \"Peter Garfiel Freeman writes:>>them. (By the way, I do not applaud the killing of _any_ human being,>>including prisoners sentenced to death by our illustrious justice department)>>>>Peace.>>-marc>Boy, you really are a stupid person.  Our justice department does>not sentence people to death.  That's up to state courts.  Again,>get a brain.Peter, I think you are ridiculous here. Stupidity is not a measure of howwell someone knows our judicial system. I guess Marc meant that he is against death penalty. But no matter what he meant, your statement not justified.Regards, \",\n",
       " 'doc_topics': [(10, 0.02670317006560966),\n",
       "  (2, 0.01921959019789326),\n",
       "  (5, 0.014685307597847946),\n",
       "  (4, 0.00932246214555893),\n",
       "  (14, 0.007307856551774964)],\n",
       " 'kl_summary': \"  Our justice department does>not sentence people to death. Peter, I think you are ridiculous here.  Stupidity is not a measure of howwell someone knows our judicial system.  But no matter what he meant, your statement not justified. Regards, .   That's up to state courts.   Again,>get a brain. Peter Garfiel Freeman writes:>>them. >>-marc>Boy, you really are a stupid person.  I guess Marc meant that he is against death penalty. \"}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_id': '17318',\n",
       " 'doc_text': '>I am considering buying Borland\\'s Paradox for Windows since I>would like to use a database with Windows (I don\\'t have/use>one yet) for both work/home use.  I would like to advantage>of Borland\\'s \"$129.95 until April 30\" offer if this package>is everything that Borland claims it to be.  So, I was>wondering ... has anybody used this and/or have any opinions?>>-- Tom BelmonteIf you are interested in a program which is very easy to use, I strongly suggest Approach 2.0.  It is extremely easy to use, make reports, etc.  Iown both it and Paradox, and I almost never use Paradox.  If you need to build up a complicated application, then Paradox is the way to go.  I haveheard horror stories about the Access programming being extremely cryptic.Since you seem like you will probably be doing fairly small stuff (work/home use and you have not used a database before), I recommend Approach.  I have found only one small thing which I would like it to do more easily:I have one database where the order in which the records are entered must be different than what is the logical ordering.  To permanently reorder (to use the old DBASE III command) the records requires sorting the records appropriately (no problem, since I almost always use them in this order)exporting the database to another database (which can still be an Approach database), and then copying the exported files back to the original filename.  This is a small weakness, considering the other items I really like about Approach.  It is also a little slower than Paradox (other than the loading, Paradox takes forever and a minute to load).  Paradox also takes a lot of memory (both hard disk (around 12MB) and RAM).',\n",
       " 'doc_topics': [(3, 0.07736179283617246),\n",
       "  (7, 0.05725053905196663),\n",
       "  (13, 0.04460855520033822),\n",
       "  (10, 0.031556746202370804),\n",
       "  (5, 0.02536669134289394)],\n",
       " 'kl_summary': ' has anybody used this and/or have any opinions?>>-- Tom BelmonteIf you are interested in a program which is very easy to use, I strongly suggest Approach 2.   It is also a little slower than Paradox (other than the loading, Paradox takes forever and a minute to load). 95 until April 30\" offer if this package>is everything that Borland claims it to be. . . 0. .   I would like to advantage>of Borland\\'s \"$129.   So, I was>wondering .   Paradox also takes a lot of memory (both hard disk (around 12MB) and RAM). '}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True,\n",
       " 'index': 'newsgroup_topic_modelling',\n",
       " 'shards_acknowledged': True}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch('localhost')\n",
    "es.indices.delete(index='newsgroup_topic_modelling', ignore=[400, 404])\n",
    "mappings_duc = {\n",
    "    'mappings':{\n",
    "        'words':{\n",
    "            'properties':{\n",
    "                'topic_id': {'type': 'text', 'index': 'false'},\n",
    "                'top_words': {'type': 'text', 'analyzer': 'english'},\n",
    "                'word_probs':{'type': 'text', 'analyzer': 'english'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "es.indices.create(index=\"newsgroup_topic_modelling\", body=mappings_duc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = []\n",
    "for i in range(len(topic_words_dict)):\n",
    "    topics.append({\n",
    "        'topic_id' : str(i),\n",
    "        'top_words': topic_words_dict[i],\n",
    "        'word_probs': topic_words_prob_dict[i]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in topics:\n",
    "    es.index(index='newsgroup_topic_modelling', doc_type='words', body=topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'index': 'newsgroup', 'shards_acknowledged': True}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.delete(index='newsgroup', ignore=[400, 404])\n",
    "mappings_duc = {\n",
    "    'mappings':{\n",
    "        'document':{\n",
    "            'properties':{\n",
    "                'doc_id': {'type': 'text', 'index': 'false'},\n",
    "                'doc_text': {'type': 'text', 'analyzer': 'english'},\n",
    "                'kl_summary':{'type': 'text', 'analyzer': 'english'},\n",
    "                'doc_topics' : {'type': 'text', 'analyzer': 'english'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "es.indices.create(index=\"newsgroup\", body=mappings_duc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    es.index(index='newsgroup', doc_type='document', body=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch(['http://localhost:9200/'])\n",
    "doc = {\n",
    "        'size' : 10301,\n",
    "        'query': {\n",
    "            'match_all' : {}\n",
    "       }\n",
    "   }\n",
    "res = es.search(index='duc', doc_type='document', body=doc,scroll='1m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'_source'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-7f5c0e4873b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m'gold_summary'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_source'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '_source'"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "count = 0\n",
    "for item in res['hits']['hits']:\n",
    "    if 'gold_summary' in item['_source']:\n",
    "        docs.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': 'P4TLp2IBFcz1SPPjAKSs', '_type': 'document'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    doc.pop('_index')\n",
    "    doc.pop('_score')\n",
    "    doc.pop('_source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': 'P4TLp2IBFcz1SPPjAKSs', '_type': 'document'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'index': 'duc', 'shards_acknowledged': True}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.delete(index='duc', ignore=[400, 404])\n",
    "mappings_duc = {\n",
    "    'mappings':{\n",
    "        'document':{\n",
    "            'properties':{\n",
    "                'doc_id': {'type': 'text', 'index': 'false'},\n",
    "                'doc_text': {'type': 'text', 'analyzer': 'english'},\n",
    "                'gold_summary':{'type': 'text', 'analyzer': 'english'},\n",
    "                'doc_topics':{'type': 'text', 'analyzer': 'english'},\n",
    "                'lda_summary':{'type': 'text', 'analyzer': 'english'},\n",
    "                'kl_summary':{'type': 'text', 'analyzer': 'english'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "es.indices.create(index=\"duc\", body=mappings_duc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "POST http://localhost:9200/duc/document [status:400 request:0.148s]\n"
     ]
    },
    {
     "ename": "RequestError",
     "evalue": "TransportError(400, 'mapper_parsing_exception', 'Field [_index] is a metadata field and cannot be added inside a document. Use the index API request parameters.')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRequestError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-ed6e8bd6aac9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'duc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'document'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/elasticsearch/client/utils.py\u001b[0m in \u001b[0;36m_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/elasticsearch/client/__init__.py\u001b[0m in \u001b[0;36mindex\u001b[0;34m(self, index, doc_type, body, id, params)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty value passed for a required argument.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         return self.transport.perform_request('POST' if id in SKIP_IN_PATH else 'PUT',\n\u001b[0;32m--> 319\u001b[0;31m             _make_path(index, doc_type, id), params=params, body=body)\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     @query_params('_source', '_source_exclude', '_source_include', 'parent',\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/elasticsearch/transport.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m                 \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTransportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/elasticsearch/connection/http_urllib3.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_request_fail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         self.log_request_success(method, full_url, url, body, response.status,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/elasticsearch/connection/base.py\u001b[0m in \u001b[0;36m_raise_error\u001b[0;34m(self, status_code, raw_data)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Undecodable raw error response from server: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTP_EXCEPTIONS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransportError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRequestError\u001b[0m: TransportError(400, 'mapper_parsing_exception', 'Field [_index] is a metadata field and cannot be added inside a document. Use the index API request parameters.')"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    es.index(index='duc', doc_type='document', body=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
