{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        topic_words_dict[topic_idx] = message.split()[2:] \n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 2.215s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "done in 0.602s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 0.492s.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.363s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "Topic #0: just people don think like know time good make way really say right ve want did ll new use years\n",
      "Topic #1: windows use dos using window program os drivers application help software pc running ms screen files version card code work\n",
      "Topic #2: god jesus bible faith christian christ christians does heaven sin believe lord life church mary atheism belief human love religion\n",
      "Topic #3: thanks know does mail advance hi info interested email anybody looking card help like appreciated information send list video need\n",
      "Topic #4: car cars tires miles 00 new engine insurance price condition oil power speed good 000 brake year models used bought\n",
      "Topic #5: edu soon com send university internet mit ftp mail cc pub article information hope program mac email home contact blood\n",
      "Topic #6: file problem files format win sound ftp pub read save site help image available create copy running memory self version\n",
      "Topic #7: game team games year win play season players nhl runs goal hockey toronto division flyers player defense leafs bad teams\n",
      "Topic #8: drive drives hard disk floppy software card mac computer power scsi controller apple mb 00 pc rom sale problem internal\n",
      "Topic #9: key chip clipper keys encryption government public use secure enforcement phone nsa communications law encrypted security clinton used legal standard\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=2000 and n_features=1000...\n",
      "done in 2.270s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
      "Topic #0: people just like time don say really know way things make think right said did want ve probably work years\n",
      "Topic #1: windows thanks using help need hi work know use looking mail software does used pc video available running info advance\n",
      "Topic #2: god does true read know say believe subject says religion mean question point jesus people book christian mind understand matter\n",
      "Topic #3: thanks know like interested mail just want new send edu list does bike thing email reply post wondering hear heard\n",
      "Topic #4: time new 10 year sale old offer 20 16 15 great 30 weeks good test model condition 11 14 power\n",
      "Topic #5: use number com government new university data states information talk phone right including security provide control following long used research\n",
      "Topic #6: edu try file soon remember problem com program hope mike space article wrong library short include win little couldn sun\n",
      "Topic #7: year world team game play won win games season maybe case second does did series playing nhl fact said points\n",
      "Topic #8: think don drive need hard make people mac read going pretty try sure order means trying apple case bit drives\n",
      "Topic #9: just good use way got like ll doesn want sure don doing thought does wrong right better make stuff speed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n",
      "done in 5.020s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: edu com mail send graphics ftp pub available contact university list faq ca information cs 1993 program sun uk mit\n",
      "Topic #1: don like just know think ve way use right good going make sure ll point got need really time doesn\n",
      "Topic #2: christian think atheism faith pittsburgh new bible radio games alt lot just religion like book read play time subject believe\n",
      "Topic #3: drive disk windows thanks use card drives hard version pc software file using scsi help does new dos controller 16\n",
      "Topic #4: hiv health aids disease april medical care research 1993 light information study national service test led 10 page new drug\n",
      "Topic #5: god people does just good don jesus say israel way life know true fact time law want believe make think\n",
      "Topic #6: 55 10 11 18 15 team game 19 period play 23 12 13 flyers 20 25 22 17 24 16\n",
      "Topic #7: car year just cars new engine like bike good oil insurance better tires 000 thing speed model brake driving performance\n",
      "Topic #8: people said did just didn know time like went think children came come don took years say dead told started\n",
      "Topic #9: key space law government public use encryption earth section security moon probe enforcement keys states lunar military crime surface technology\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "tf_feature_probs = tf_vectorizer.max_features\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n",
      "done in 6.977s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit_transform(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: edu com mail send graphics ftp pub available contact university list faq ca information cs 1993 program sun uk mit\n",
      "Topic #1: don like just know think ve way use right good going make sure ll point got need really time doesn\n",
      "Topic #2: christian think atheism faith pittsburgh new bible radio games alt lot just religion like book read play time subject believe\n",
      "Topic #3: drive disk windows thanks use card drives hard version pc software file using scsi help does new dos controller 16\n",
      "Topic #4: hiv health aids disease april medical care research 1993 light information study national service test led 10 page new drug\n",
      "Topic #5: god people does just good don jesus say israel way life know true fact time law want believe make think\n",
      "Topic #6: 55 10 11 18 15 team game 19 period play 23 12 13 flyers 20 25 22 17 24 16\n",
      "Topic #7: car year just cars new engine like bike good oil insurance better tires 000 thing speed model brake driving performance\n",
      "Topic #8: people said did just didn know time like went think children came come don took years say dead told started\n",
      "Topic #9: key space law government public use encryption earth section security moon probe enforcement keys states lunar military crime surface technology\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words_prob_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  0 :  [0.05201295519414298, 0.023301467982438023, 0.018757677200607382, 0.017853034313902293, 0.01595098191416809, 0.01421039583681441, 0.013452740553974264, 0.011952686203526814, 0.011686121720430662, 0.010583275527159389, 0.009855197508471327, 0.009576367273422747, 0.009509668275976298, 0.009114667086822948, 0.008586300586097065, 0.007287622462962518, 0.007127524628490003, 0.007015770896936109, 0.006898763186196211, 0.006855242806574529]\n",
      "Topic  1 :  [0.022526318730000276, 0.019200694844516055, 0.016172949248556064, 0.014221316124639125, 0.013672203283395343, 0.012894625646772535, 0.012293395990878996, 0.010477578387284935, 0.010473732540938498, 0.010437073090386372, 0.010000710949059643, 0.009012636990721816, 0.008548999170659056, 0.008421857190456418, 0.007740498647604117, 0.007739467458600744, 0.007157696679956578, 0.007051879181552583, 0.006808575881926617, 0.006753129655481801]\n",
      "Topic  2 :  [0.03246905623669139, 0.030531335701191153, 0.023140364283488854, 0.0193858904471216, 0.01817496181548329, 0.0135474521683227, 0.012346358804736664, 0.012129767290121116, 0.01189965287866572, 0.011750142035273234, 0.011145137683166344, 0.010765512537948217, 0.010116957728371415, 0.010105665773441103, 0.010076011879763855, 0.009959037749786192, 0.009676920518741597, 0.009492143037036133, 0.009481177889449362, 0.009433693676928714]\n",
      "Topic  3 :  [0.015516269149138989, 0.0133786070253682, 0.013345319294017489, 0.01153645903265467, 0.010955628400982097, 0.010884922758572883, 0.010398867753195923, 0.010017039189285837, 0.009621048127250433, 0.00959667340055702, 0.009314515114636228, 0.009251301945711258, 0.008947799625303992, 0.008090028420419924, 0.007612923476948574, 0.007534996359861904, 0.007354339295584869, 0.007339615728733459, 0.007227521590175311, 0.007192320787630595]\n",
      "Topic  4 :  [0.04538636522325106, 0.04382437724813909, 0.034725327482252204, 0.032023906974168445, 0.02998201631837523, 0.023207250065698664, 0.02203876656319334, 0.018011406473397978, 0.01692268417139174, 0.01633495529551431, 0.014808652818737562, 0.01419849261717119, 0.014183231569545849, 0.014144282843589605, 0.013811272972371464, 0.013240263307603027, 0.011543443309850203, 0.011012917207197374, 0.010445494935250394, 0.010305987471381204]\n",
      "Topic  5 :  [0.028927387630506528, 0.020787420495545477, 0.016518880733500864, 0.012383647776586324, 0.01055399699070941, 0.010154237011733623, 0.009258791410284803, 0.008802591781035358, 0.008539410267511722, 0.008447843935264028, 0.00840419549432924, 0.008005056942658828, 0.007449137793979705, 0.007436820947732387, 0.007424829669905564, 0.007322407176817621, 0.007216852841411122, 0.007209266153507506, 0.0071900797808325985, 0.0071180255462695505]\n",
      "Topic  6 :  [0.03624489908653221, 0.031025866731314123, 0.023371132851261633, 0.01733462582096894, 0.016549562455125736, 0.016477099193474804, 0.016265775698587184, 0.01564843024650582, 0.015464257311821817, 0.014637002583728367, 0.014502786903071403, 0.014371251133076574, 0.014319934662895398, 0.014179926605727223, 0.013901917876087208, 0.01378460341224643, 0.013465639297657676, 0.013159126337140593, 0.012889442076335706, 0.012869072242481214]\n",
      "Topic  7 :  [0.033147878167394465, 0.01667497971493912, 0.013580647259594769, 0.012632691772703031, 0.012364674116600695, 0.011969428392882092, 0.011734922267757172, 0.011617683897852966, 0.011018187476231485, 0.010992962541528801, 0.010942372330143151, 0.009306154860057, 0.008453567530117704, 0.007799358058219848, 0.007360395338669536, 0.007261139034026595, 0.0072593043413142905, 0.006948373401909811, 0.006884700391143453, 0.006754972356779644]\n",
      "Topic  8 :  [0.02834940218341225, 0.017361347275918343, 0.012759418215518657, 0.01218991230712469, 0.012187553150987938, 0.011355992104509893, 0.010828687871058616, 0.010657804867414302, 0.008708429294101776, 0.008481283843052075, 0.007353767400852352, 0.006874973880332953, 0.006836683596194964, 0.006799498061834044, 0.006603338276447843, 0.006423153681972147, 0.006269409693950501, 0.005768094022600923, 0.005601205260814968, 0.005597655068715193]\n",
      "Topic  9 :  [0.021873213250490044, 0.021159670246099563, 0.014953148884934306, 0.013066710525171887, 0.012551211642513372, 0.011222350615433729, 0.0103892612698724, 0.01013981172250363, 0.00961731494460912, 0.009480113526455655, 0.00842482527335372, 0.007515303452619633, 0.007461716182060763, 0.007461348803765604, 0.007373454932830264, 0.006928014204271913, 0.006836891385254658, 0.006577038059926782, 0.006569127142455631, 0.00638830422918811]\n"
     ]
    }
   ],
   "source": [
    "lda.components_ /= lda.components_.sum(axis=1)[:, np.newaxis]\n",
    "for i in range(10):\n",
    "    topic_words_prob_dict[i] = sorted(lda.components_[i])[::-1][:20] \n",
    "    print(\"Topic \",i,\": \", sorted(lda.components_[i])[::-1][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True,\n",
       " 'index': 'newsgroup_topic_modelling',\n",
       " 'shards_acknowledged': True}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "client = Elasticsearch('localhost')\n",
    "mappings_duc = {\n",
    "    'mappings':{\n",
    "        'words':{\n",
    "            'properties':{\n",
    "                'topic_id': {'type': 'text', 'index': 'false'},\n",
    "                'top_words': {'type': 'text', 'analyzer': 'english'},\n",
    "                'word_probs':{'type': 'text', 'analyzer': 'english'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "client.indices.create(index=\"newsgroup_topic_modelling\", body=mappings_duc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = []\n",
    "for i in range(len(topic_words_dict)):\n",
    "    topics.append({\n",
    "        'topic_id' : str(i),\n",
    "        'top_words': topic_words_dict[i],\n",
    "        'word_probs': topic_words_prob_dict[i]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in topics:\n",
    "    client.index(index='newsgroup_topic_modelling', doc_type='words', body=topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf.inverse_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch(['http://localhost:9200/'])\n",
    "doc = {\n",
    "        'size': 10000,\n",
    "        'query': {\n",
    "            'match_all' : {}\n",
    "       }\n",
    "   }\n",
    "res = es.search(index='newsgroup', doc_type='document', body=doc)\n",
    "docs = []\n",
    "for item in res['hits']['hits']:\n",
    "    docs.append(item['_source']['doc_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_docs = tf_vectorizer.inverse_transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for word_list in transformed_docs:\n",
    "    text = \"\"\n",
    "    for word in word_list:\n",
    "        text += word + \" \"\n",
    "    documents.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'information material reference manual user ths couldn colours height width stored data mean like look format bitmap windows does exactly wrote tcd unix2 robertsa roberts andrew '"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "from itertools import chain\n",
    "from nltk.corpus import stopwords\n",
    "# remove common words and tokenize\n",
    "# stoplist = set('for a of the and to in is that i have this not you are was it not be with have'.split())\n",
    "# stoplist = set(stopwords.words('english'))\n",
    "# texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "#          for document in documents]\n",
    "texts = [[word for word in document.lower().split()]\n",
    "         for document in documents]\n",
    "# remove words that appear only once\n",
    "all_tokens = sum(texts, [])\n",
    "tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)\n",
    "texts = [[word for word in text if word not in tokens_once] for text in texts]\n",
    "\n",
    "# Create Dictionary.\n",
    "id2word = corpora.Dictionary(texts)\n",
    "# Creates the Bag of Word corpus.\n",
    "mm = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the LDA models.\n",
    "lda = models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_corpus = lda[mm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lda_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 0.014916044), (17, 0.97508794)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.003*\"said\" + 0.003*\"year\" + 0.002*\"time\" + 0.002*\"people\" + 0.002*\"years\" + 0.002*\"000\" + 0.002*\"government\" + 0.002*\"state\" + 0.002*\"like\" + 0.001*\"just\" + 0.001*\"according\" + 0.001*\"say\" + 0.001*\"states\" + 0.001*\"20\" + 0.001*\"called\" + 0.001*\"did\" + 0.001*\"officials\" + 0.001*\"house\" + 0.001*\"new\" + 0.001*\"today\" + 0.001*\"times\" + 0.001*\"10\" + 0.001*\"way\" + 0.001*\"long\" + 0.001*\"50\" + 0.001*\"use\" + 0.001*\"home\" + 0.001*\"president\" + 0.001*\"going\" + 0.001*\"american\" + 0.001*\"number\" + 0.001*\"day\" + 0.001*\"world\" + 0.001*\"mr\" + 0.001*\"100\" + 0.001*\"total\" + 0.001*\"expected\" + 0.001*\"far\" + 0.001*\"million\" + 0.001*\"based\" + 0.001*\"taken\" + 0.001*\"week\" + 0.001*\"record\" + 0.001*\"away\" + 0.001*\"near\" + 0.001*\"general\" + 0.001*\"old\" + 0.001*\"nation\" + 0.001*\"members\" + 0.001*\"national\" + 0.001*\"high\" + 0.001*\"second\" + 0.001*\"miles\" + 0.001*\"public\" + 0.001*\"don\" + 0.001*\"right\" + 0.001*\"come\" + 0.001*\"30\" + 0.001*\"took\" + 0.001*\"department\"'),\n",
       " (1,\n",
       "  '0.003*\"said\" + 0.003*\"new\" + 0.002*\"people\" + 0.002*\"year\" + 0.002*\"government\" + 0.002*\"way\" + 0.002*\"time\" + 0.002*\"officials\" + 0.002*\"state\" + 0.002*\"000\" + 0.002*\"years\" + 0.002*\"second\" + 0.002*\"called\" + 0.002*\"going\" + 0.001*\"like\" + 0.001*\"10\" + 0.001*\"week\" + 0.001*\"just\" + 0.001*\"including\" + 0.001*\"say\" + 0.001*\"national\" + 0.001*\"says\" + 0.001*\"city\" + 0.001*\"did\" + 0.001*\"according\" + 0.001*\"told\" + 0.001*\"50\" + 0.001*\"long\" + 0.001*\"force\" + 0.001*\"country\" + 0.001*\"states\" + 0.001*\"world\" + 0.001*\"support\" + 0.001*\"federal\" + 0.001*\"record\" + 0.001*\"miles\" + 0.001*\"30\" + 0.001*\"president\" + 0.001*\"service\" + 0.001*\"group\" + 0.001*\"mr\" + 0.001*\"don\" + 0.001*\"congress\" + 0.001*\"make\" + 0.001*\"know\" + 0.001*\"work\" + 0.001*\"control\" + 0.001*\"day\" + 0.001*\"used\" + 0.001*\"house\" + 0.001*\"past\" + 0.001*\"near\" + 0.001*\"american\" + 0.001*\"member\" + 0.001*\"washington\" + 0.001*\"took\" + 0.001*\"public\" + 0.001*\"life\" + 0.001*\"100\" + 0.001*\"number\"'),\n",
       " (2,\n",
       "  '0.005*\"said\" + 0.003*\"year\" + 0.002*\"time\" + 0.002*\"new\" + 0.002*\"years\" + 0.002*\"people\" + 0.002*\"make\" + 0.002*\"national\" + 0.002*\"000\" + 0.001*\"ago\" + 0.001*\"just\" + 0.001*\"world\" + 0.001*\"according\" + 0.001*\"officials\" + 0.001*\"states\" + 0.001*\"like\" + 0.001*\"near\" + 0.001*\"government\" + 0.001*\"expected\" + 0.001*\"20\" + 0.001*\"10\" + 0.001*\"center\" + 0.001*\"million\" + 0.001*\"did\" + 0.001*\"south\" + 0.001*\"american\" + 0.001*\"caused\" + 0.001*\"damage\" + 0.001*\"report\" + 0.001*\"says\" + 0.001*\"state\" + 0.001*\"high\" + 0.001*\"early\" + 0.001*\"week\" + 0.001*\"general\" + 0.001*\"say\" + 0.001*\"chief\" + 0.001*\"mr\" + 0.001*\"including\" + 0.001*\"group\" + 0.001*\"going\" + 0.001*\"1988\" + 0.001*\"told\" + 0.001*\"mexico\" + 0.001*\"information\" + 0.001*\"spokesman\" + 0.001*\"does\" + 0.001*\"today\" + 0.001*\"killed\" + 0.001*\"called\" + 0.001*\"based\" + 0.001*\"50\" + 0.001*\"small\" + 0.001*\"later\" + 0.001*\"city\" + 0.001*\"used\" + 0.001*\"service\" + 0.001*\"air\" + 0.001*\"west\" + 0.001*\"think\"'),\n",
       " (3,\n",
       "  '0.003*\"people\" + 0.003*\"said\" + 0.003*\"year\" + 0.002*\"national\" + 0.002*\"time\" + 0.002*\"state\" + 0.002*\"president\" + 0.002*\"new\" + 0.002*\"years\" + 0.001*\"just\" + 0.001*\"country\" + 0.001*\"000\" + 0.001*\"public\" + 0.001*\"high\" + 0.001*\"states\" + 0.001*\"including\" + 0.001*\"according\" + 0.001*\"miles\" + 0.001*\"million\" + 0.001*\"world\" + 0.001*\"say\" + 0.001*\"city\" + 0.001*\"end\" + 0.001*\"ofthe\" + 0.001*\"officials\" + 0.001*\"hours\" + 0.001*\"day\" + 0.001*\"make\" + 0.001*\"times\" + 0.001*\"12\" + 0.001*\"federal\" + 0.001*\"10\" + 0.001*\"called\" + 0.001*\"service\" + 0.001*\"says\" + 0.001*\"government\" + 0.001*\"did\" + 0.001*\"reported\" + 0.001*\"like\" + 0.001*\"north\" + 0.001*\"mr\" + 0.001*\"right\" + 0.001*\"began\" + 0.001*\"long\" + 0.001*\"work\" + 0.001*\"number\" + 0.001*\"100\" + 0.001*\"1988\" + 0.001*\"second\" + 0.001*\"west\" + 0.001*\"average\" + 0.001*\"office\" + 0.001*\"area\" + 0.001*\"later\" + 0.001*\"months\" + 0.001*\"early\" + 0.001*\"away\" + 0.001*\"200\" + 0.001*\"march\" + 0.001*\"support\"'),\n",
       " (4,\n",
       "  '0.004*\"said\" + 0.003*\"years\" + 0.002*\"year\" + 0.002*\"people\" + 0.002*\"state\" + 0.002*\"time\" + 0.002*\"new\" + 0.002*\"national\" + 0.002*\"officials\" + 0.002*\"government\" + 0.002*\"just\" + 0.001*\"president\" + 0.001*\"area\" + 0.001*\"states\" + 0.001*\"long\" + 0.001*\"including\" + 0.001*\"make\" + 0.001*\"million\" + 0.001*\"like\" + 0.001*\"day\" + 0.001*\"way\" + 0.001*\"country\" + 0.001*\"according\" + 0.001*\"south\" + 0.001*\"says\" + 0.001*\"30\" + 0.001*\"expected\" + 0.001*\"world\" + 0.001*\"20\" + 0.001*\"near\" + 0.001*\"service\" + 0.001*\"say\" + 0.001*\"called\" + 0.001*\"law\" + 0.001*\"house\" + 0.001*\"did\" + 0.001*\"miles\" + 0.001*\"work\" + 0.001*\"man\" + 0.001*\"home\" + 0.001*\"10\" + 0.001*\"don\" + 0.001*\"high\" + 0.001*\"public\" + 0.001*\"june\" + 0.001*\"major\" + 0.001*\"000\" + 0.001*\"chief\" + 0.001*\"old\" + 0.001*\"little\" + 0.001*\"congress\" + 0.001*\"city\" + 0.001*\"areas\" + 0.001*\"control\" + 0.001*\"right\" + 0.001*\"set\" + 0.001*\"office\" + 0.001*\"told\" + 0.001*\"big\" + 0.001*\"west\"'),\n",
       " (5,\n",
       "  '0.003*\"year\" + 0.003*\"said\" + 0.002*\"people\" + 0.002*\"national\" + 0.002*\"new\" + 0.002*\"time\" + 0.002*\"officials\" + 0.002*\"government\" + 0.002*\"years\" + 0.002*\"state\" + 0.002*\"just\" + 0.002*\"10\" + 0.002*\"says\" + 0.002*\"000\" + 0.002*\"service\" + 0.001*\"department\" + 0.001*\"day\" + 0.001*\"president\" + 0.001*\"public\" + 0.001*\"say\" + 0.001*\"world\" + 0.001*\"american\" + 0.001*\"like\" + 0.001*\"week\" + 0.001*\"50\" + 0.001*\"did\" + 0.001*\"called\" + 0.001*\"make\" + 0.001*\"including\" + 0.001*\"spokesman\" + 0.001*\"killed\" + 0.001*\"center\" + 0.001*\"million\" + 0.001*\"today\" + 0.001*\"city\" + 0.001*\"20\" + 0.001*\"north\" + 0.001*\"going\" + 0.001*\"number\" + 0.001*\"federal\" + 0.001*\"early\" + 0.001*\"miles\" + 0.001*\"night\" + 0.001*\"monday\" + 0.001*\"used\" + 0.001*\"use\" + 0.001*\"old\" + 0.001*\"caused\" + 0.001*\"local\" + 0.001*\"country\" + 0.001*\"west\" + 0.001*\"told\" + 0.001*\"high\" + 0.001*\"work\" + 0.001*\"way\" + 0.001*\"does\" + 0.001*\"members\" + 0.001*\"14\" + 0.001*\"coast\" + 0.001*\"right\"'),\n",
       " (6,\n",
       "  '0.003*\"said\" + 0.003*\"years\" + 0.002*\"people\" + 0.002*\"year\" + 0.002*\"just\" + 0.002*\"new\" + 0.002*\"officials\" + 0.002*\"time\" + 0.002*\"national\" + 0.002*\"high\" + 0.002*\"state\" + 0.001*\"10\" + 0.001*\"according\" + 0.001*\"000\" + 0.001*\"government\" + 0.001*\"country\" + 0.001*\"mr\" + 0.001*\"20\" + 0.001*\"like\" + 0.001*\"city\" + 0.001*\"states\" + 0.001*\"south\" + 0.001*\"long\" + 0.001*\"public\" + 0.001*\"president\" + 0.001*\"15\" + 0.001*\"area\" + 0.001*\"did\" + 0.001*\"day\" + 0.001*\"service\" + 0.001*\"center\" + 0.001*\"say\" + 0.001*\"way\" + 0.001*\"world\" + 0.001*\"members\" + 0.001*\"says\" + 0.001*\"called\" + 0.001*\"miles\" + 0.001*\"spokesman\" + 0.001*\"number\" + 0.001*\"30\" + 0.001*\"force\" + 0.001*\"began\" + 0.001*\"department\" + 0.001*\"don\" + 0.001*\"house\" + 0.001*\"far\" + 0.001*\"million\" + 0.001*\"nation\" + 0.001*\"later\" + 0.001*\"largest\" + 0.001*\"congress\" + 0.001*\"told\" + 0.001*\"early\" + 0.001*\"case\" + 0.001*\"use\" + 0.001*\"50\" + 0.001*\"old\" + 0.001*\"want\" + 0.001*\"washington\"'),\n",
       " (7,\n",
       "  '0.003*\"said\" + 0.003*\"year\" + 0.003*\"people\" + 0.002*\"new\" + 0.002*\"state\" + 0.002*\"officials\" + 0.002*\"000\" + 0.002*\"years\" + 0.002*\"just\" + 0.002*\"10\" + 0.001*\"time\" + 0.001*\"including\" + 0.001*\"million\" + 0.001*\"don\" + 0.001*\"going\" + 0.001*\"say\" + 0.001*\"department\" + 0.001*\"mr\" + 0.001*\"support\" + 0.001*\"long\" + 0.001*\"miles\" + 0.001*\"world\" + 0.001*\"does\" + 0.001*\"federal\" + 0.001*\"city\" + 0.001*\"government\" + 0.001*\"president\" + 0.001*\"california\" + 0.001*\"30\" + 0.001*\"national\" + 0.001*\"states\" + 0.001*\"early\" + 0.001*\"like\" + 0.001*\"used\" + 0.001*\"work\" + 0.001*\"high\" + 0.001*\"past\" + 0.001*\"make\" + 0.001*\"force\" + 0.001*\"help\" + 0.001*\"100\" + 0.001*\"11\" + 0.001*\"public\" + 0.001*\"end\" + 0.001*\"american\" + 0.001*\"major\" + 0.001*\"damage\" + 0.001*\"control\" + 0.001*\"region\" + 0.001*\"service\" + 0.001*\"want\" + 0.001*\"congress\" + 0.001*\"result\" + 0.001*\"second\" + 0.001*\"set\" + 0.001*\"20\" + 0.001*\"total\" + 0.001*\"old\" + 0.001*\"times\" + 0.001*\"director\"'),\n",
       " (8,\n",
       "  '0.003*\"said\" + 0.003*\"people\" + 0.002*\"new\" + 0.002*\"national\" + 0.002*\"say\" + 0.002*\"year\" + 0.002*\"time\" + 0.002*\"state\" + 0.001*\"just\" + 0.001*\"like\" + 0.001*\"president\" + 0.001*\"including\" + 0.001*\"city\" + 0.001*\"early\" + 0.001*\"government\" + 0.001*\"members\" + 0.001*\"day\" + 0.001*\"old\" + 0.001*\"million\" + 0.001*\"director\" + 0.001*\"used\" + 0.001*\"000\" + 0.001*\"number\" + 0.001*\"think\" + 0.001*\"based\" + 0.001*\"way\" + 0.001*\"according\" + 0.001*\"officials\" + 0.001*\"use\" + 0.001*\"20\" + 0.001*\"says\" + 0.001*\"10\" + 0.001*\"called\" + 0.001*\"near\" + 0.001*\"make\" + 0.001*\"months\" + 0.001*\"took\" + 0.001*\"long\" + 0.001*\"police\" + 0.001*\"american\" + 0.001*\"week\" + 0.001*\"did\" + 0.001*\"30\" + 0.001*\"york\" + 0.001*\"control\" + 0.001*\"service\" + 0.001*\"added\" + 0.001*\"expected\" + 0.001*\"right\" + 0.001*\"states\" + 0.001*\"center\" + 0.001*\"mr\" + 0.001*\"federal\" + 0.001*\"department\" + 0.001*\"going\" + 0.001*\"chief\" + 0.001*\"free\" + 0.001*\"country\" + 0.001*\"believe\" + 0.001*\"member\"'),\n",
       " (9,\n",
       "  '0.004*\"said\" + 0.003*\"year\" + 0.002*\"years\" + 0.002*\"people\" + 0.002*\"new\" + 0.002*\"000\" + 0.002*\"10\" + 0.002*\"time\" + 0.001*\"national\" + 0.001*\"states\" + 0.001*\"government\" + 0.001*\"just\" + 0.001*\"going\" + 0.001*\"state\" + 0.001*\"including\" + 0.001*\"officials\" + 0.001*\"say\" + 0.001*\"president\" + 0.001*\"went\" + 0.001*\"long\" + 0.001*\"city\" + 0.001*\"week\" + 0.001*\"mr\" + 0.001*\"high\" + 0.001*\"today\" + 0.001*\"day\" + 0.001*\"million\" + 0.001*\"way\" + 0.001*\"like\" + 0.001*\"federal\" + 0.001*\"south\" + 0.001*\"second\" + 0.001*\"20\" + 0.001*\"expected\" + 0.001*\"free\" + 0.001*\"service\" + 0.001*\"think\" + 0.001*\"country\" + 0.001*\"1988\" + 0.001*\"end\" + 0.001*\"north\" + 0.001*\"did\" + 0.001*\"damage\" + 0.001*\"old\" + 0.001*\"make\" + 0.001*\"center\" + 0.001*\"later\" + 0.001*\"don\" + 0.001*\"washington\" + 0.001*\"world\" + 0.001*\"far\" + 0.001*\"used\" + 0.001*\"june\" + 0.001*\"30\" + 0.001*\"department\" + 0.001*\"ago\" + 0.001*\"14\" + 0.001*\"public\" + 0.001*\"hours\" + 0.001*\"right\"')]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(num_topics=10, num_words=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.003*\"said\" + 0.003*\"year\" + 0.002*\"time\" + 0.002*\"people\" + 0.002*\"years\" + 0.002*\"000\" + 0.002*\"government\" + 0.002*\"state\" + 0.002*\"like\" + 0.001*\"just\" + 0.001*\"according\" + 0.001*\"say\" + 0.001*\"states\" + 0.001*\"20\" + 0.001*\"called\" + 0.001*\"did\" + 0.001*\"officials\" + 0.001*\"house\" + 0.001*\"new\" + 0.001*\"today\"'),\n",
       " (1,\n",
       "  '0.003*\"said\" + 0.003*\"new\" + 0.002*\"people\" + 0.002*\"year\" + 0.002*\"government\" + 0.002*\"way\" + 0.002*\"time\" + 0.002*\"officials\" + 0.002*\"state\" + 0.002*\"000\" + 0.002*\"years\" + 0.002*\"second\" + 0.002*\"called\" + 0.002*\"going\" + 0.001*\"like\" + 0.001*\"10\" + 0.001*\"week\" + 0.001*\"just\" + 0.001*\"including\" + 0.001*\"say\"'),\n",
       " (2,\n",
       "  '0.005*\"said\" + 0.003*\"year\" + 0.002*\"time\" + 0.002*\"new\" + 0.002*\"years\" + 0.002*\"people\" + 0.002*\"make\" + 0.002*\"national\" + 0.002*\"000\" + 0.001*\"ago\" + 0.001*\"just\" + 0.001*\"world\" + 0.001*\"according\" + 0.001*\"officials\" + 0.001*\"states\" + 0.001*\"like\" + 0.001*\"near\" + 0.001*\"government\" + 0.001*\"expected\" + 0.001*\"20\"'),\n",
       " (3,\n",
       "  '0.003*\"people\" + 0.003*\"said\" + 0.003*\"year\" + 0.002*\"national\" + 0.002*\"time\" + 0.002*\"state\" + 0.002*\"president\" + 0.002*\"new\" + 0.002*\"years\" + 0.001*\"just\" + 0.001*\"country\" + 0.001*\"000\" + 0.001*\"public\" + 0.001*\"high\" + 0.001*\"states\" + 0.001*\"including\" + 0.001*\"according\" + 0.001*\"miles\" + 0.001*\"million\" + 0.001*\"world\"'),\n",
       " (4,\n",
       "  '0.004*\"said\" + 0.003*\"years\" + 0.002*\"year\" + 0.002*\"people\" + 0.002*\"state\" + 0.002*\"time\" + 0.002*\"new\" + 0.002*\"national\" + 0.002*\"officials\" + 0.002*\"government\" + 0.002*\"just\" + 0.001*\"president\" + 0.001*\"area\" + 0.001*\"states\" + 0.001*\"long\" + 0.001*\"including\" + 0.001*\"make\" + 0.001*\"million\" + 0.001*\"like\" + 0.001*\"day\"'),\n",
       " (5,\n",
       "  '0.003*\"year\" + 0.003*\"said\" + 0.002*\"people\" + 0.002*\"national\" + 0.002*\"new\" + 0.002*\"time\" + 0.002*\"officials\" + 0.002*\"government\" + 0.002*\"years\" + 0.002*\"state\" + 0.002*\"just\" + 0.002*\"10\" + 0.002*\"says\" + 0.002*\"000\" + 0.002*\"service\" + 0.001*\"department\" + 0.001*\"day\" + 0.001*\"president\" + 0.001*\"public\" + 0.001*\"say\"'),\n",
       " (6,\n",
       "  '0.003*\"said\" + 0.003*\"years\" + 0.002*\"people\" + 0.002*\"year\" + 0.002*\"just\" + 0.002*\"new\" + 0.002*\"officials\" + 0.002*\"time\" + 0.002*\"national\" + 0.002*\"high\" + 0.002*\"state\" + 0.001*\"10\" + 0.001*\"according\" + 0.001*\"000\" + 0.001*\"government\" + 0.001*\"country\" + 0.001*\"mr\" + 0.001*\"20\" + 0.001*\"like\" + 0.001*\"city\"'),\n",
       " (7,\n",
       "  '0.003*\"said\" + 0.003*\"year\" + 0.003*\"people\" + 0.002*\"new\" + 0.002*\"state\" + 0.002*\"officials\" + 0.002*\"000\" + 0.002*\"years\" + 0.002*\"just\" + 0.002*\"10\" + 0.001*\"time\" + 0.001*\"including\" + 0.001*\"million\" + 0.001*\"don\" + 0.001*\"going\" + 0.001*\"say\" + 0.001*\"department\" + 0.001*\"mr\" + 0.001*\"support\" + 0.001*\"long\"'),\n",
       " (8,\n",
       "  '0.003*\"said\" + 0.003*\"people\" + 0.002*\"new\" + 0.002*\"national\" + 0.002*\"say\" + 0.002*\"year\" + 0.002*\"time\" + 0.002*\"state\" + 0.001*\"just\" + 0.001*\"like\" + 0.001*\"president\" + 0.001*\"including\" + 0.001*\"city\" + 0.001*\"early\" + 0.001*\"government\" + 0.001*\"members\" + 0.001*\"day\" + 0.001*\"old\" + 0.001*\"million\" + 0.001*\"director\"'),\n",
       " (9,\n",
       "  '0.004*\"said\" + 0.003*\"year\" + 0.002*\"years\" + 0.002*\"people\" + 0.002*\"new\" + 0.002*\"000\" + 0.002*\"10\" + 0.002*\"time\" + 0.001*\"national\" + 0.001*\"states\" + 0.001*\"government\" + 0.001*\"just\" + 0.001*\"going\" + 0.001*\"state\" + 0.001*\"including\" + 0.001*\"officials\" + 0.001*\"say\" + 0.001*\"president\" + 0.001*\"went\" + 0.001*\"long\"')]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.show_topics(num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
