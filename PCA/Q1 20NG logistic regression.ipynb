{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecthing data..................................\n",
      "Data fetched successfully\n",
      "Turning data into Tf-IDF format................\n",
      "Successfully created bag of words.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fecthing data..................................\")\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "newsgroups_data = newsgroups_train.data + newsgroups_test.data\n",
    "\n",
    "print(\"Data fetched successfully\")\n",
    "print(\"Turning data into Tf-IDF format................\")\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(newsgroups_data)\n",
    "\n",
    "print(\"Successfully created bag of words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18846, 173762)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.fit(vectors[:11314], newsgroups_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = log_reg.predict(vectors[11314:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of predictions:  82.38183749336166 %\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(newsgroups_test['target'],predicted_labels)\n",
    "print(\"Accuracy of predictions: \" ,score * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class  1 :\n",
      "Top 30 features:\n",
      "[ 96998  41862  41867  51730  92243  92240 107248 120183  80391 102414\n",
      "  93350 138405  92132 106531  45328 134805  68505 142241 157772 165852\n",
      "  98266 121500 106631  55887 151845  81294 145221  88360  45934 140562]\n",
      "===================================================================================\n",
      "Class  2 :\n",
      "Top 30 features:\n",
      "[ 81086  89197  17778 127099 153398  89214  60219  74763 126945  17817\n",
      "  76153 127590  39499 122892 146008 161755  79646  56259 128822 161641\n",
      "  38333  74738 149381  18957 101780  55949 131768 145115  13302 112710]\n",
      "===================================================================================\n",
      "Class  3 :\n",
      "Top 30 features:\n",
      "[165812  74738  66522  42770  66525  74763  54685  66002 112567 165761\n",
      " 118351  90576 136561 113153 165763  75907  75922 128515  77074  36078\n",
      " 159672 109955 128822  52154  47046 159609  63921 106455 128653  63297]\n",
      "===================================================================================\n",
      "Class  4 :\n",
      "Top 30 features:\n",
      "[ 66513 141006  52154  49287 124071  88481  78570  57900 112019  19688\n",
      " 127325 162396  46263 112445  64759  92147  75520  92038  68978 112023\n",
      "  84521  66002  47144  88238  66527 152565 108844  58835  47516 150210]\n",
      "===================================================================================\n",
      "Class  5 :\n",
      "Top 30 features:\n",
      "[105625  40259 131540  67065  53222 141162 127616 100734 100772 112019\n",
      " 143536  88943 111721  36625  51035  47055  66513 143529 118430 162908\n",
      "  23225  55524 159289 105758  91294 105840 128653 132816 145920  76405]\n",
      "===================================================================================\n",
      "Class  6 :\n",
      "Top 30 features:\n",
      "[165803 112457 141960 165557 110771 167479 169512 100788 168805  40291\n",
      "  72380 169109  64890  55949 148952 167464 138361 165559  56259 132283\n",
      " 101702  55409  56282  89244  70286 168331  55405  91308 128822 126128]\n",
      "===================================================================================\n",
      "Class  7 :\n",
      "Top 30 features:\n",
      "[139570  75985 142771 119890  76209 141600      0  57143 128421  91213\n",
      "  41433 164208 116508  51672 154750  78348  65086 106121  89856 119897\n",
      "  71971  69514 141605 129895 119441  85346  52935 120056  48035 121248]\n",
      "===================================================================================\n",
      "Class  8 :\n",
      "Top 30 features:\n",
      "[ 52100  52328  70105 120104  61933  76018  42438 154592  42371  51693\n",
      "  69331  86078  42459  56335 164012  47826 164279  67008 114471 140023\n",
      " 158598  66535 163183  53181 137166 130729  57970 142831  91067 155521]\n",
      "===================================================================================\n",
      "Class  9 :\n",
      "Top 30 features:\n",
      "[ 46061  65684  46066 112496  47058 136354 136375  84511 112497 136357\n",
      "  65723  56335 114471 132892  86078  38790  47117 111633  83643 148952\n",
      "  45079  76882  86461 146965  68358  90280 170534  68035  66897  91030]\n",
      "===================================================================================\n",
      "Class  10 :\n",
      "Top 30 features:\n",
      "[ 44285  84147 125429 126442  59899 170770 138366 126060  48088 151673\n",
      "  68505 145576  78332  43895  38595 109247  85412 146953  78348  94048\n",
      "  85440 153412  73504 165761  85369 100441 170555  93592 158511 117360]\n",
      "===================================================================================\n",
      "Class  11 :\n",
      "Top 30 features:\n",
      "[ 85770 116858 151673  78332  51412 126433 126457 100881 126442  80345\n",
      " 141263  60020 151681  84147  63510 126458 126093 124549  71123 165868\n",
      "  63603 132932  88310  75633  78348 124512 154365  55842 112139  35665]\n",
      "===================================================================================\n",
      "Class  12 :\n",
      "Top 30 features:\n",
      "[ 55485  97220  69924  54193 118280  97274  81836 125218 141388  59550\n",
      " 151236  63171  80766  59561 141348  71043  55949 131574  68641  69921\n",
      " 116311  56335 128559  38333 165992 115253  91183  47432 152015 129661]\n",
      "===================================================================================\n",
      "Class  13 :\n",
      "Top 30 features:\n",
      "[ 54823  69234 127608 132590 162680 156118  39051  42158  81523 132621\n",
      "  63437  58051  60071  54827 159609 112510  68535 102119 125497 165978\n",
      "  28145 121808  43523  86482  63256  36630 103246 159966 140774 133849]\n",
      "===================================================================================\n",
      "Class  14 :\n",
      "Top 30 features:\n",
      "[113242  65652 126081  64682 108523  78886  44041  80653  67394 155155\n",
      " 123005  75928  51881  84216 142535 108530 123830  75931 125567 140691\n",
      " 172839  64063  90429 119845  92132 150110 108480  72601 150060 170783]\n",
      "===================================================================================\n",
      "Class  15 :\n",
      "Top 30 features:\n",
      "[145613 112164 115537 121078 100531  84631  38170  64161 123761 143120\n",
      " 103793 140680  68002 145620  36078  61756 127884  75433 137311  42276\n",
      " 145153 153296 173160 118319 107813  66712 144104  85159  77286  64079]\n",
      "===================================================================================\n",
      "Class  16 :\n",
      "Top 30 features:\n",
      "[ 80391  54556 138451  54432  41895  54402  54421  54429  94017  55387\n",
      "  84475  45934 164701 119845   8896  52576  40424 125001 143613  73383\n",
      "  68038  79076 135701  75033 140957 139322 106948  84290 121692  93590]\n",
      "===================================================================================\n",
      "Class  17 :\n",
      "Top 30 features:\n",
      "[ 82049  82085  73833  74957 164738 163961  44412  41850 148009  74364\n",
      "  74954 118199  83374 164736 110207 136438  80766  53003  38896  74956\n",
      "  56335  88663  80340  83378 132888 157480  56264 100592 149475 157484]\n",
      "===================================================================================\n",
      "Class  18 :\n",
      "Top 30 features:\n",
      "[ 92333  92334 156026  94054  40956  40954  40953  40578 141884 156021\n",
      "  40803 107925  93403 119845 145171  94048  58417  58824  39459  76321\n",
      " 156037  40591  85942  36778 127011  56321  36574 164981 172817  91942]\n",
      "===================================================================================\n",
      "Class  19 :\n",
      "Top 30 features:\n",
      "[ 55458  96377  58952 151416 120927  66615 128277  78644  80766  47941\n",
      "  55278  94727  92165 147217  84216  91824 142134  56335  66486  83204\n",
      " 153971 155238  48540 137509  62408  91030 152049 138451 125379 166913]\n",
      "===================================================================================\n",
      "Class  20 :\n",
      "Top 30 features:\n",
      "[ 54421 139794 112224  98651 119393  94017  80391  86969  45938  48255\n",
      "  48932 164889 137635  97102 121124 119845  76561  53529 171315 156452\n",
      " 112298  94943  38873  38362  44845 110800  94291 153284 154216 165435]\n",
      "===================================================================================\n"
     ]
    }
   ],
   "source": [
    "importances = log_reg.coef_\n",
    "feature = 1\n",
    "for coefs in importances:\n",
    "    print(\"Class \",feature, \":\")\n",
    "    idx = coefs.argsort()[::-1]   \n",
    "    print(\"Top 30 features:\")\n",
    "    print(idx[:30])\n",
    "    print(\"===================================================================================\")\n",
    "    feature += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
