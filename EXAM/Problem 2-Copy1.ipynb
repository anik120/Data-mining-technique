{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn import mixture\n",
    "import itertools, operator\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "n_top_words = 50\n",
    "n_clusters = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_docs = []\n",
    "whole_doc_labels = []\n",
    "whole_doc_id = []\n",
    "file = open(\"whole_dataset.txt\", 'r')\n",
    "for line in file:\n",
    "    line = ast.literal_eval(line)\n",
    "    whole_doc_id.append(line[0])\n",
    "    whole_doc_labels.append(line[1])\n",
    "    whole_docs.append(line[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "doc_topics = []\n",
    "doc_id = []\n",
    "file = open(\"sample_dataset.txt\", 'r')\n",
    "for line in file:\n",
    "    line = ast.literal_eval(line)\n",
    "    doc_id.append(line[0])\n",
    "    doc_topics.append(line[1])\n",
    "    docs.append(line[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        topic_words_dict[topic_idx] = message.split()[2:] \n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(whole_docs)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting LDA models with tf features...\")\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: 10 25 12 15 20 11 00 14 000 16 13 30 17 18 19 team 50 40 arab 28 22 new 26 21 23 24 27 season 31 russia vs jehovah play 29 35 33 60 league power 34 38 1st 32 80 det 42 series 37 45 75\n",
      "Topic #1: people don just think like know time said did say good way right does make believe ve going government years really want point year new things law ll didn fact come question let day mr life long state president use better sure children used thing got doesn true case course\n",
      "Topic #2: myers cancer adl pts gm bullock lebanon dee w7 cx chronic clinical 17 pitching sky c_ liver corpses uw ck nutrition hz t7 cubs chz ss defeated b6 lk sp breast embargo defensive gant defamation w1 24 pms providence magnesium pitchers leaf hitter calcium bc ron smokeless al alomar a7\n",
      "Topic #3: edu space com university 1993 nasa mail list information available ftp april cs pub send ca research server sun san army 1992 national news anonymous 93 gov contact mit pit address center subject internet 92 boston file email conference washington shuttle apr new faq motif net request based republic los\n",
      "Topic #4: ax max g9v b8f a86 pl 145 1d9 34u 1t 0t 75u 3t giz bhj 2di 2tm wm 7ey candida bxn 0d sl 6ei gk anonymity 6um 9v bj qq yeast infected 34 04 ahl hiv m3 qax tq wt 45 okz 3l g9 intercourse b4q ql 5u kn tm\n",
      "Topic #5: la chi bh ar bd van ah mov min km ra tb kt sj si pt al s1 bm amp cx kk rw s2 db cs air 65 ma agents 66 64 sb 34 mtl 68 mm cf rm ott um p2 7u en sy fm hm 71 nyr rf\n",
      "Topic #6: windows dos window win driver problem os mouse run 02 03 running card drivers orbit fonts manager grace msg use won microsoft 04 font com 01 using memory program printer nt ram application shell pitt mode sys xterm gordon simms mission engines 05 error skepticism soon shameful works runs tcp\n",
      "Topic #7: use file data program like key used does know using software thanks need drive bit new work information chip available number time computer just help image want don mail version files code line pc make disk problem encryption set systems color good mac package ms ve standard card run email\n",
      "Topic #8: god jesus armenian israel armenians jews christ turkish church bible christian jewish lord christians greek turkey armenia faith history genocide turks word azerbaijan men holy peace heaven spirit shall ottoman arabs dead christianity villages istanbul world john ed professor greece killed land man azerbaijani magi argic acts serdar catholic doctrine\n",
      "Topic #9: xx period entry bos buf nhl pp output int null char define nj rules build include flyers printf year devils pittsburgh return islanders entries sabres buffalo oname la xfree86 playoff lemieux contest bruins tommy lib nyi fprintf lindros col penguins joke linux sh hawks winnipeg title modes red das leafs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words_prob_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  0 :  [0.01759491971046672, 0.01239994974952365, 0.011806640988006557, 0.011583959365739515, 0.011437053197477983, 0.009939797385691245, 0.009776647072210085, 0.008902063714394181, 0.0086912192712741, 0.008519816139045252, 0.0077730891261401855, 0.007713252322532368, 0.007433130664784713, 0.007073472182527312, 0.006536247391303389, 0.0064022001357547645, 0.006295022963667286, 0.0059347417254066205, 0.0056630506800525995, 0.005504146842298651, 0.005467368788261871, 0.0054462600424007015, 0.0053613379331516175, 0.005315948081883595, 0.004780863109826516, 0.004639703480735854, 0.004595891629704095, 0.004154248494753469, 0.004041491589903295, 0.0037998996997051226, 0.0037099413467352664, 0.0036107124082551826, 0.00360747992271911, 0.0035860753364310707, 0.0034738252979943872, 0.0034340857105170097, 0.003268632956535425, 0.003210099694204126, 0.0030524408306140637, 0.0030456444282097625, 0.002912344884513777, 0.002888929677520292, 0.0028316779229265445, 0.002791236220568835, 0.0027492887032991398, 0.0027367607518496637, 0.0027239268798464177, 0.0025370418751050096, 0.0025035957276519655, 0.0024577633261760805]\n",
      "Topic  1 :  [0.0076797471716502546, 0.006317672904998178, 0.005519391097166351, 0.0052952953783734205, 0.005080262099771514, 0.004785574549655337, 0.0042255143325797655, 0.003628309267078525, 0.003613720057507726, 0.0034238087757434326, 0.0032900021196132826, 0.003150476159485432, 0.00314204157102012, 0.0029918678152831453, 0.0027212448347876802, 0.0025389826414211887, 0.002487879478496175, 0.0024837731707401944, 0.002408350362461855, 0.0023408923309772905, 0.002170230075322704, 0.0021197392717578504, 0.002100143039906416, 0.002073874164542213, 0.001963972116710798, 0.00195792230548412, 0.0019557225272195914, 0.0019202438477853637, 0.0018952154645583928, 0.0018873786388308412, 0.0018468941321079146, 0.0017956393544016477, 0.0017896947389233987, 0.0017142119579165655, 0.0016986878285917598, 0.0016648357187503448, 0.001657556665239444, 0.0015840139606019257, 0.0015827529076023845, 0.0015521222807238816, 0.0015507331271664255, 0.0015482915458708377, 0.0015238093845178005, 0.0015151666336876762, 0.0014846385914200851, 0.0014840174148851558, 0.0014778623353814597, 0.0014766059908716028, 0.0014744055640519616, 0.0014377105887781931]\n",
      "Topic  2 :  [0.017685328557370372, 0.012542544409605842, 0.009808705441330627, 0.008032082059758937, 0.0063018859236015565, 0.005987119869772414, 0.005029954976922773, 0.004452354458257884, 0.004436004294312937, 0.004338843055506523, 0.003820685376812936, 0.0036980484114215533, 0.003642316299192006, 0.003589376013300783, 0.003434097818676743, 0.0032034019514180115, 0.0030896841844991066, 0.0029752973022300305, 0.002910491389966169, 0.00289532171371153, 0.0027880287317872928, 0.0027870380266560033, 0.002767645983974696, 0.002738018074410348, 0.002584874064032496, 0.0024818324991346063, 0.0024450881517401163, 0.002345685978307048, 0.0023452214825686554, 0.002341683245440821, 0.002318375969815967, 0.002164295781916159, 0.0021615689714940705, 0.0021585792316981007, 0.0021009129716467625, 0.0019368352503844257, 0.0019264906417965858, 0.0019154344937061909, 0.0019064432376962228, 0.001827674305752004, 0.0018198878533978201, 0.0018029385979479947, 0.0017991952525998013, 0.0017887606025816009, 0.0017594123989389328, 0.0017358602611668732, 0.001693121406047532, 0.001683552299542486, 0.0016341936444826842, 0.0016265252748596172]\n",
      "Topic  3 :  [0.017571673486635662, 0.008458186607737499, 0.007552426602794052, 0.00612609538087696, 0.005039676986842235, 0.004807483470806456, 0.004733699005338137, 0.004695509378669361, 0.0044753869803130005, 0.004325510875654802, 0.0038492083844024368, 0.003732351026344533, 0.003538228433759687, 0.003523244555006552, 0.0034624825303938636, 0.003430016657051817, 0.0032618172349510284, 0.003232535912802363, 0.00320042184292045, 0.0028296160626517268, 0.0028263844609835564, 0.0028159169857606965, 0.0028071492315784575, 0.0027221356154868525, 0.002677325909559989, 0.0025588507898343625, 0.0025580731358824816, 0.0024730940930466363, 0.0024086987204613958, 0.0022521481343779183, 0.0022385680632551423, 0.002210863077928839, 0.00218399558919207, 0.0021787702076448293, 0.002069705186832274, 0.00204009809560454, 0.002017839274062757, 0.0020102786944433374, 0.0019880899191643464, 0.0019350968400379624, 0.0019315800505706987, 0.0018608714847789176, 0.0018474677843567714, 0.001842102312387588, 0.0018418873081255819, 0.0018251656373510113, 0.0017490671371421798, 0.0016845544986017614, 0.0016805813577815477, 0.0016748781729661378]\n",
      "Topic  4 :  [0.5225059920248628, 0.03894847885194468, 0.00949436455642558, 0.009166458465286397, 0.006998422187736479, 0.006367450549333436, 0.005839514385535169, 0.004898102874140237, 0.0040800453140471815, 0.003940325109813063, 0.0038849222688613007, 0.0036609658180762113, 0.0035337345088774686, 0.0034249594943739766, 0.0033904939830051556, 0.003386551933789467, 0.0029313830169153824, 0.0028004331287275674, 0.0024470426493206476, 0.00222309392912148, 0.001999492912286452, 0.0019092075326960834, 0.0017649661885064216, 0.0017577077602331835, 0.0017525775303754816, 0.0016654667048894446, 0.0016496325497277968, 0.0016353423925267903, 0.0016133415672203587, 0.001553074424818624, 0.0012829849984527667, 0.0012466227524945577, 0.001132609174548834, 0.0011188274024193261, 0.0010883035159910252, 0.0010762099814314186, 0.0010662295723460858, 0.001064817376167032, 0.0010431791490290287, 0.0010350759456776767, 0.0010270828871290134, 0.001014510306908291, 0.0010102709640886821, 0.0009981595740465078, 0.0009965560483448286, 0.0009921952180071453, 0.0009819848587863287, 0.0009571620658358559, 0.00095083496223701, 0.00093448989451175]\n",
      "Topic  5 :  [0.00411556335958979, 0.003959937044667291, 0.0038777564602449845, 0.003820516238621143, 0.00357730834568045, 0.003548208026272605, 0.002801385770799176, 0.002747027552715048, 0.002737207197078597, 0.002609063477465814, 0.002430717806803391, 0.0024197247898050133, 0.002390529414629559, 0.002281151643366399, 0.002269044022132614, 0.002249356427323022, 0.0022310790605357387, 0.0022070808849876354, 0.0020944138419356603, 0.002059511485257769, 0.002042660186516314, 0.002032416410371706, 0.0019819861929847417, 0.0019342745495495213, 0.0019264164593661608, 0.0018072020850110639, 0.0017809290446312795, 0.0017788599443199025, 0.0017440383259506444, 0.0017307284706986619, 0.0016837346131981844, 0.0016827663142324337, 0.0016645130367374573, 0.0016476124431310615, 0.0016287447583267442, 0.0016268304270865904, 0.0015954677177028895, 0.0015803961800695068, 0.001579289530973017, 0.0015781030343689235, 0.00154949592350379, 0.001518350879058196, 0.0015157785560946642, 0.0015024073006760664, 0.0014871102593298335, 0.0014757189226171068, 0.00145856879706074, 0.0014242382013283912, 0.0014129094396294996, 0.0013937640518560163]\n",
      "Topic  6 :  [0.023612276750118753, 0.020441314244328032, 0.010969196446508225, 0.007111286681612568, 0.006874541466635897, 0.006405542341208083, 0.006047530134365178, 0.005993739866487302, 0.005476664580342777, 0.005399603737109215, 0.005285489631076352, 0.005244803099106041, 0.005239990693076468, 0.005207229948987043, 0.004970974363700707, 0.004739292579358335, 0.00466247187707479, 0.004606487994794124, 0.004606005484898952, 0.004593500385341912, 0.0042412758792413794, 0.004048615020312138, 0.003832684786875375, 0.00377553898368016, 0.0037728281182260407, 0.003565207305755295, 0.003501394782857769, 0.003434761935474332, 0.0032620874454905202, 0.003254690300982311, 0.0032253888213993357, 0.003216876785273602, 0.003145988249043598, 0.0031413974272918406, 0.0030872834829489644, 0.0029622406514407526, 0.0027433221931907285, 0.002684105663142489, 0.002661366715065068, 0.002655063124464869, 0.002588114117623658, 0.0025746784408790186, 0.0025570998126029356, 0.0025309370481019678, 0.002500810090164322, 0.0024495478781966407, 0.0024049332051896653, 0.0023775995623633622, 0.0023175315304994715, 0.002305076241044593]\n",
      "Topic  7 :  [0.008196322084342893, 0.005011962057252141, 0.004577531711429562, 0.004428307338127934, 0.004370529439258391, 0.004306949640219754, 0.004151032682588346, 0.004072140717648239, 0.0037487252748947546, 0.003736599598877208, 0.003578725214307616, 0.003470245024905156, 0.0034036976350341375, 0.0033493355034098704, 0.003337304194896504, 0.00314332685814846, 0.0031091735100438303, 0.0029143724252757327, 0.002713827328251776, 0.002700914396741589, 0.0026845766261804124, 0.0026139537298449883, 0.0026096702847488128, 0.0025972350810114222, 0.002573025085628406, 0.0025603602554646657, 0.002541604697006537, 0.0024897409106610963, 0.002349511410989966, 0.0023286123766273685, 0.0022785209028681874, 0.002244328234444015, 0.002161923788850278, 0.0021429679207419666, 0.002054806399251139, 0.002046165160488734, 0.002024484982777726, 0.001994035140352793, 0.0019881182131541316, 0.0019443709020562387, 0.001916104460718928, 0.0019121435323837589, 0.0018998139003218134, 0.001851004934530911, 0.0018247042564677569, 0.0017914298164726646, 0.001779301337444659, 0.0017679661308978721, 0.0017431885566079972, 0.001742137617599768]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  8 :  [0.036350925693910664, 0.018372743238581016, 0.016467145286651382, 0.014015326785038291, 0.013916554056279666, 0.013723905699069429, 0.01203855252142939, 0.011254615152027558, 0.010663310945355062, 0.01047462746408932, 0.00939567020506815, 0.00909319518131555, 0.00854076540116057, 0.007899162103010442, 0.007151413655867338, 0.007043315863715094, 0.00610130259681285, 0.005958506856854531, 0.005827274989926142, 0.005602447287507066, 0.005348430430120089, 0.004840755090846378, 0.004550326119156347, 0.004230322737468553, 0.004196040276320416, 0.004087142053309006, 0.004039578469829165, 0.003991824895136781, 0.003891748405973123, 0.0036065613978433727, 0.0034949675830049766, 0.0034716554920020066, 0.003455000152157379, 0.0034258162580423895, 0.00340296475520623, 0.0033956067042299255, 0.0033628427141547575, 0.0031163157219543255, 0.0030729426661569004, 0.003026671615697633, 0.0030118338535545294, 0.002819157630207219, 0.0028106648906197616, 0.002802753371472271, 0.0027612208948634394, 0.0027577965685054865, 0.002745641055989318, 0.0026562597437436823, 0.002555569991998617, 0.0025508976785570416]\n",
      "Topic  9 :  [0.013264160901431536, 0.010851349454094276, 0.008576960392438724, 0.007932056255625557, 0.0076587348190339626, 0.007174543380842759, 0.00646867151042318, 0.006310680855431303, 0.005228495585786914, 0.004206806067869808, 0.0040535168641913765, 0.0038744857302282615, 0.0036341584828969036, 0.003614103008104786, 0.00357403402123582, 0.00357163310417907, 0.0035079889154344458, 0.003422800880609811, 0.0034079480782181443, 0.003319410830184126, 0.003309328510189722, 0.0032324074925746517, 0.0032178683138575992, 0.0031952440774684512, 0.0031055885806556644, 0.003027245612876109, 0.002968002440492621, 0.0029122389031806337, 0.002881324120551255, 0.0028430660594018055, 0.002789213278581558, 0.0027866475918812248, 0.0026233979938713307, 0.0026128968518779787, 0.002509810751720794, 0.0024857539129393906, 0.002476188436632085, 0.002476051505408124, 0.002475637477113197, 0.0024674746569570277, 0.002379428294391257, 0.0023602315618853373, 0.0023019308167873845, 0.002095357256144096, 0.002063071938262779, 0.0020389569549522837, 0.0019723521391043844, 0.0019682370699292376, 0.0019578275517901306, 0.001941192870942774]\n"
     ]
    }
   ],
   "source": [
    "lda.components_ /= lda.components_.sum(axis=1)[:, np.newaxis]\n",
    "for i in range(n_components):\n",
    "    topic_words_prob_dict[i] = sorted(lda.components_[i])[::-1][:n_top_words] \n",
    "    print(\"Topic \",i,\": \", sorted(lda.components_[i])[::-1][:n_top_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")\n",
    "def predict_doc_topic(doc, topic_words_dict, topic_words_prob_dict):\n",
    "    text = nltk.tokenize.word_tokenize(doc)\n",
    "#     for word in text:\n",
    "#         if not d.check(word):\n",
    "#             if len(word) < 4:\n",
    "#                 text.remove(word)\n",
    "    temp_dist = nltk.FreqDist(text)\n",
    "    word_dist = {}\n",
    "    for word in temp_dist:\n",
    "        word_dist[word] = temp_dist[word]\n",
    "#     for word in temp_dist:\n",
    "#         if word_dist[word] > 10:\n",
    "#             word_dist.pop(word)\n",
    "    doc_topics_prob = {}\n",
    "    for topic in topic_words_dict:\n",
    "        score = 0\n",
    "        word_list = topic_words_dict[topic]\n",
    "        prob_list = topic_words_prob_dict[topic]\n",
    "        for index in range(len(word_list)):\n",
    "            if word_list[index] in word_dist:\n",
    "                score += word_dist[word_list[index]] * prob_list[index]\n",
    "        doc_topics_prob[topic] = score\n",
    "    return doc_topics_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vectors = []\n",
    "for doc in docs:\n",
    "    doc_topic_probs = predict_doc_topic(doc, topic_words_dict, topic_words_prob_dict)\n",
    "    vector = []\n",
    "    for i in range(n_components):\n",
    "        vector.append(doc_topic_probs[i])\n",
    "    new_vectors.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vectors = np.array(new_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4491, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Mixture object\n",
      "Done fitting data\n"
     ]
    }
   ],
   "source": [
    "clf = mixture.GaussianMixture(n_components=n_clusters, covariance_type='full')\n",
    "print(\"Creating Mixture object\")\n",
    "clf.fit(new_vectors)\n",
    "print(\"Done fitting data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict_proba(new_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_label(l):\n",
    "    it = itertools.groupby(l, operator.itemgetter(1))\n",
    "    counts = []\n",
    "    for key, subiter in it:\n",
    "        counts.append(sum(item[0] for item in subiter))\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_homogeneity(preds, labels):\n",
    "    cluster_label_counts = []\n",
    "    for pred in preds.transpose():\n",
    "        cluster_label_counts.append(group_by_label([(p,label) for p,label in zip(pred,labels)]))\n",
    "    \n",
    "    entropys = []\n",
    "    for cluster_label_count in cluster_label_counts:\n",
    "        entropys.append(scipy.stats.entropy(cluster_label_count))\n",
    "         \n",
    "    return np.mean(entropys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_completeness(preds, labels, num_clusters, num_labels):\n",
    "    label_cluster_counts = {label:np.zeros(num_clusters) for label in range(num_labels)}\n",
    "    \n",
    "    for pred, label in zip(preds, labels):\n",
    "        label_cluster_counts[label] = np.sum([label_cluster_counts[label], pred], axis=0)\n",
    "    \n",
    "    entropys = []\n",
    "    for label_cluster_count in label_cluster_counts.values():\n",
    "        entropys.append(scipy.stats.entropy(label_cluster_count))\n",
    "          \n",
    "    return np.mean(entropys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_measure(preds, labels, num_clusters, num_labels):\n",
    "    if len(labels) == 0:\n",
    "        return 1.0, 1.0, 1.0\n",
    "      \n",
    "    homogeneity = compute_homogeneity(preds, labels)\n",
    "    completeness = compute_completeness(preds, labels, num_clusters, num_labels)\n",
    "    \n",
    "    if homogeneity==0.0 and completeness==0.0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    v_measure_score = (2.0 * homogeneity * completeness /\n",
    "                   (homogeneity + completeness))\n",
    "      \n",
    "    return homogeneity, completeness, v_measure_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_label_mapping = {}\n",
    "label = 0\n",
    "for topic in set(doc_topics):\n",
    "    topic_label_mapping[topic] = label\n",
    "    label += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for topic in doc_topics:\n",
    "    labels.append(topic_label_mapping[topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.1674909457219775, 0.8323492689969504, 0.9718378779264868)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(v_measure(preds, labels, num_clusters, num_labels))\n",
    "v_measure(pred, labels, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4491, 10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in double_scalars\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "soft_cluster_size = [0 for i in range(n_clusters)]\n",
    "for p in pred:\n",
    "    for i in range(len(p)):\n",
    "        soft_cluster_size[i] += soft_cluster_size[i] + p[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(soft_cluster_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = -1\n",
    "max_size = -1234214141241431414\n",
    "for cluster_size in soft_cluster_size:\n",
    "    if cluster_size > max_size:\n",
    "        max_size = cluster_size\n",
    "        max_index = soft_cluster_size.index(cluster_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = clf.predict(new_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_indexs_in_cluster_0 = []\n",
    "for index in range(len(cluster_labels)):\n",
    "    if cluster_labels[index] == 0:\n",
    "        doc_indexs_in_cluster_0.append(index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 3,\n",
       " 4,\n",
       " 9,\n",
       " 10,\n",
       " 16,\n",
       " 18,\n",
       " 28,\n",
       " 30,\n",
       " 31,\n",
       " 33,\n",
       " 34,\n",
       " 37,\n",
       " 44,\n",
       " 54,\n",
       " 57,\n",
       " 59,\n",
       " 60,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 67,\n",
       " 73,\n",
       " 77,\n",
       " 78,\n",
       " 83,\n",
       " 91,\n",
       " 95,\n",
       " 98,\n",
       " 104,\n",
       " 105,\n",
       " 109,\n",
       " 110,\n",
       " 115,\n",
       " 124,\n",
       " 126,\n",
       " 128,\n",
       " 129,\n",
       " 132,\n",
       " 138,\n",
       " 143,\n",
       " 146,\n",
       " 149,\n",
       " 151,\n",
       " 152,\n",
       " 159,\n",
       " 187,\n",
       " 201,\n",
       " 206,\n",
       " 209,\n",
       " 217,\n",
       " 234,\n",
       " 268,\n",
       " 285,\n",
       " 293,\n",
       " 298,\n",
       " 317,\n",
       " 341,\n",
       " 346,\n",
       " 360,\n",
       " 378,\n",
       " 385,\n",
       " 388,\n",
       " 399,\n",
       " 403,\n",
       " 414,\n",
       " 431,\n",
       " 448,\n",
       " 467,\n",
       " 473,\n",
       " 521,\n",
       " 567,\n",
       " 621,\n",
       " 640,\n",
       " 653,\n",
       " 700,\n",
       " 727,\n",
       " 824,\n",
       " 843,\n",
       " 905,\n",
       " 945,\n",
       " 1001,\n",
       " 1035,\n",
       " 1073,\n",
       " 1076,\n",
       " 1099,\n",
       " 1108,\n",
       " 1122,\n",
       " 1133,\n",
       " 1140,\n",
       " 1152,\n",
       " 1155,\n",
       " 1164,\n",
       " 1172,\n",
       " 1177,\n",
       " 1190,\n",
       " 1219,\n",
       " 1227,\n",
       " 1252,\n",
       " 1260,\n",
       " 1266,\n",
       " 1273,\n",
       " 1327,\n",
       " 1338,\n",
       " 1349,\n",
       " 1371,\n",
       " 1394,\n",
       " 1408,\n",
       " 1434,\n",
       " 1523,\n",
       " 1537,\n",
       " 1574,\n",
       " 1607,\n",
       " 1641,\n",
       " 1665,\n",
       " 1686,\n",
       " 1690,\n",
       " 1694,\n",
       " 1712,\n",
       " 1716,\n",
       " 1717,\n",
       " 1719,\n",
       " 1721,\n",
       " 1728,\n",
       " 1731,\n",
       " 1733,\n",
       " 1751,\n",
       " 1762,\n",
       " 1764,\n",
       " 1765,\n",
       " 1766,\n",
       " 1767,\n",
       " 1768,\n",
       " 1769,\n",
       " 1779,\n",
       " 1781,\n",
       " 1788,\n",
       " 1798,\n",
       " 1799,\n",
       " 1800,\n",
       " 1807,\n",
       " 1809,\n",
       " 1815,\n",
       " 1821,\n",
       " 1828,\n",
       " 1834,\n",
       " 1848,\n",
       " 1849,\n",
       " 1853,\n",
       " 1855,\n",
       " 1859,\n",
       " 1864,\n",
       " 1866,\n",
       " 1870,\n",
       " 1873,\n",
       " 1874,\n",
       " 1879,\n",
       " 1882,\n",
       " 1884,\n",
       " 1892,\n",
       " 1893,\n",
       " 1898,\n",
       " 1899,\n",
       " 1905,\n",
       " 1908,\n",
       " 1910,\n",
       " 1916,\n",
       " 1918,\n",
       " 1922,\n",
       " 1928,\n",
       " 1936,\n",
       " 1965,\n",
       " 1977,\n",
       " 1992,\n",
       " 2029,\n",
       " 2038,\n",
       " 2041,\n",
       " 2042,\n",
       " 2045,\n",
       " 2061,\n",
       " 2071,\n",
       " 2088,\n",
       " 2093,\n",
       " 2097,\n",
       " 2099,\n",
       " 2117,\n",
       " 2123,\n",
       " 2140,\n",
       " 2155,\n",
       " 2157,\n",
       " 2163,\n",
       " 2164,\n",
       " 2165,\n",
       " 2185,\n",
       " 2186,\n",
       " 2228,\n",
       " 2230,\n",
       " 2231,\n",
       " 2248,\n",
       " 2254,\n",
       " 2265,\n",
       " 2267,\n",
       " 2274,\n",
       " 2287,\n",
       " 2288,\n",
       " 2309,\n",
       " 2333,\n",
       " 2338,\n",
       " 2341,\n",
       " 2344,\n",
       " 2345,\n",
       " 2348,\n",
       " 2352,\n",
       " 2353,\n",
       " 2354,\n",
       " 2358,\n",
       " 2361,\n",
       " 2363,\n",
       " 2365,\n",
       " 2370,\n",
       " 2371,\n",
       " 2374,\n",
       " 2378,\n",
       " 2381,\n",
       " 2382,\n",
       " 2383,\n",
       " 2384,\n",
       " 2385,\n",
       " 2388,\n",
       " 2389,\n",
       " 2391,\n",
       " 2394,\n",
       " 2396,\n",
       " 2399,\n",
       " 2400,\n",
       " 2403,\n",
       " 2404,\n",
       " 2407,\n",
       " 2408,\n",
       " 2411,\n",
       " 2414,\n",
       " 2422,\n",
       " 2424,\n",
       " 2426,\n",
       " 2427,\n",
       " 2429,\n",
       " 2430,\n",
       " 2433,\n",
       " 2434,\n",
       " 2437,\n",
       " 2438,\n",
       " 2439,\n",
       " 2440,\n",
       " 2442,\n",
       " 2443,\n",
       " 2444,\n",
       " 2445,\n",
       " 2448,\n",
       " 2456,\n",
       " 2460,\n",
       " 2465,\n",
       " 2466,\n",
       " 2468,\n",
       " 2470,\n",
       " 2472,\n",
       " 2474,\n",
       " 2477,\n",
       " 2478,\n",
       " 2479,\n",
       " 2480,\n",
       " 2484,\n",
       " 2487,\n",
       " 2490,\n",
       " 2492,\n",
       " 2493,\n",
       " 2496,\n",
       " 2500,\n",
       " 2501,\n",
       " 2503,\n",
       " 2504,\n",
       " 2505,\n",
       " 2512,\n",
       " 2513,\n",
       " 2514,\n",
       " 2521,\n",
       " 2523,\n",
       " 2526,\n",
       " 2529,\n",
       " 2532,\n",
       " 2533,\n",
       " 2536,\n",
       " 2537,\n",
       " 2538,\n",
       " 2539,\n",
       " 2541,\n",
       " 2542,\n",
       " 2547,\n",
       " 2549,\n",
       " 2555,\n",
       " 2557,\n",
       " 2559,\n",
       " 2560,\n",
       " 2561,\n",
       " 2562,\n",
       " 2564,\n",
       " 2566,\n",
       " 2569,\n",
       " 2570,\n",
       " 2571,\n",
       " 2572,\n",
       " 2576,\n",
       " 2577,\n",
       " 2578,\n",
       " 2580,\n",
       " 2589,\n",
       " 2592,\n",
       " 2594,\n",
       " 2596,\n",
       " 2597,\n",
       " 2601,\n",
       " 2602,\n",
       " 2604,\n",
       " 2606,\n",
       " 2608,\n",
       " 2618,\n",
       " 2620,\n",
       " 2624,\n",
       " 2625,\n",
       " 2631,\n",
       " 2632,\n",
       " 2634,\n",
       " 2635,\n",
       " 2636,\n",
       " 2637,\n",
       " 2638,\n",
       " 2639,\n",
       " 2642,\n",
       " 2644,\n",
       " 2648,\n",
       " 2650,\n",
       " 2659,\n",
       " 2660,\n",
       " 2665,\n",
       " 2667,\n",
       " 2677,\n",
       " 2678,\n",
       " 2681,\n",
       " 2685,\n",
       " 2700,\n",
       " 2701,\n",
       " 2705,\n",
       " 2706,\n",
       " 2708,\n",
       " 2709,\n",
       " 2711,\n",
       " 2718,\n",
       " 2720,\n",
       " 2725,\n",
       " 2728,\n",
       " 2730,\n",
       " 2733,\n",
       " 2735,\n",
       " 2738,\n",
       " 2740,\n",
       " 2741,\n",
       " 2743,\n",
       " 2744,\n",
       " 2746,\n",
       " 2747,\n",
       " 2749,\n",
       " 2753,\n",
       " 2754,\n",
       " 2758,\n",
       " 2761,\n",
       " 2766,\n",
       " 2767,\n",
       " 2769,\n",
       " 2773,\n",
       " 2774,\n",
       " 2775,\n",
       " 2776,\n",
       " 2778,\n",
       " 2782,\n",
       " 2784,\n",
       " 2785,\n",
       " 2786,\n",
       " 2790,\n",
       " 2791,\n",
       " 2794,\n",
       " 2797,\n",
       " 2798,\n",
       " 2803,\n",
       " 2805,\n",
       " 2809,\n",
       " 2810,\n",
       " 2814,\n",
       " 2818,\n",
       " 2819,\n",
       " 2820,\n",
       " 2821,\n",
       " 2822,\n",
       " 2823,\n",
       " 2825,\n",
       " 2827,\n",
       " 2829,\n",
       " 2831,\n",
       " 2834,\n",
       " 2836,\n",
       " 2841,\n",
       " 2843,\n",
       " 2845,\n",
       " 2846,\n",
       " 2847,\n",
       " 2849,\n",
       " 2850,\n",
       " 2853,\n",
       " 2855,\n",
       " 2856,\n",
       " 2859,\n",
       " 2860,\n",
       " 2862,\n",
       " 2864,\n",
       " 2865,\n",
       " 2871,\n",
       " 2872,\n",
       " 2875,\n",
       " 2876,\n",
       " 2878,\n",
       " 2881,\n",
       " 2882,\n",
       " 2884,\n",
       " 2887,\n",
       " 2889,\n",
       " 2890,\n",
       " 2894,\n",
       " 2901,\n",
       " 2903,\n",
       " 2904,\n",
       " 2905,\n",
       " 2907,\n",
       " 2908,\n",
       " 2911,\n",
       " 2915,\n",
       " 2924,\n",
       " 2925,\n",
       " 2928,\n",
       " 2930,\n",
       " 2931,\n",
       " 2933,\n",
       " 2941,\n",
       " 2948,\n",
       " 2953,\n",
       " 2959,\n",
       " 2961,\n",
       " 2966,\n",
       " 2972,\n",
       " 2981,\n",
       " 2992,\n",
       " 2994,\n",
       " 3006,\n",
       " 3009,\n",
       " 3014,\n",
       " 3027,\n",
       " 3029,\n",
       " 3041,\n",
       " 3048,\n",
       " 3052,\n",
       " 3061,\n",
       " 3074,\n",
       " 3080,\n",
       " 3092,\n",
       " 3095,\n",
       " 3105,\n",
       " 3116,\n",
       " 3121,\n",
       " 3123,\n",
       " 3127,\n",
       " 3129,\n",
       " 3140,\n",
       " 3146,\n",
       " 3152,\n",
       " 3159,\n",
       " 3169,\n",
       " 3175,\n",
       " 3176,\n",
       " 3188,\n",
       " 3196,\n",
       " 3197,\n",
       " 3207,\n",
       " 3212,\n",
       " 3214,\n",
       " 3216,\n",
       " 3223,\n",
       " 3224,\n",
       " 3225,\n",
       " 3242,\n",
       " 3247,\n",
       " 3251,\n",
       " 3255,\n",
       " 3257,\n",
       " 3258,\n",
       " 3261,\n",
       " 3269,\n",
       " 3270,\n",
       " 3272,\n",
       " 3287,\n",
       " 3288,\n",
       " 3293,\n",
       " 3298,\n",
       " 3299,\n",
       " 3305,\n",
       " 3310,\n",
       " 3316,\n",
       " 3319,\n",
       " 3320,\n",
       " 3322,\n",
       " 3324,\n",
       " 3327,\n",
       " 3334,\n",
       " 3339,\n",
       " 3349,\n",
       " 3374,\n",
       " 3383,\n",
       " 3385,\n",
       " 3390,\n",
       " 3391,\n",
       " 3402,\n",
       " 3405,\n",
       " 3412,\n",
       " 3417,\n",
       " 3436,\n",
       " 3461,\n",
       " 3470,\n",
       " 3476,\n",
       " 3482,\n",
       " 3489,\n",
       " 3493,\n",
       " 3505,\n",
       " 3519,\n",
       " 3521,\n",
       " 3532,\n",
       " 3533,\n",
       " 3536,\n",
       " 3554,\n",
       " 3570,\n",
       " 3586,\n",
       " 3588,\n",
       " 3597,\n",
       " 3606,\n",
       " 3607,\n",
       " 3610,\n",
       " 3621,\n",
       " 3648,\n",
       " 3671,\n",
       " 3679,\n",
       " 3680,\n",
       " 3681,\n",
       " 3683,\n",
       " 3684,\n",
       " 3685,\n",
       " 3686,\n",
       " 3688,\n",
       " 3689,\n",
       " 3691,\n",
       " 3692,\n",
       " 3693,\n",
       " 3695,\n",
       " 3697,\n",
       " 3699,\n",
       " 3702,\n",
       " 3705,\n",
       " 3707,\n",
       " 3708,\n",
       " 3711,\n",
       " 3713,\n",
       " 3714,\n",
       " 3718,\n",
       " 3721,\n",
       " 3722,\n",
       " 3723,\n",
       " 3724,\n",
       " 3727,\n",
       " 3728,\n",
       " 3730,\n",
       " 3732,\n",
       " 3735,\n",
       " 3736,\n",
       " 3741,\n",
       " 3742,\n",
       " 3744,\n",
       " 3746,\n",
       " 3748,\n",
       " 3749,\n",
       " 3751,\n",
       " 3752,\n",
       " 3753,\n",
       " 3754,\n",
       " 3761,\n",
       " 3762,\n",
       " 3764,\n",
       " 3774,\n",
       " 3776,\n",
       " 3777,\n",
       " 3778,\n",
       " 3780,\n",
       " 3781,\n",
       " 3782,\n",
       " 3783,\n",
       " 3784,\n",
       " 3787,\n",
       " 3788,\n",
       " 3790,\n",
       " 3791,\n",
       " 3798,\n",
       " 3801,\n",
       " 3803,\n",
       " 3804,\n",
       " 3808,\n",
       " 3810,\n",
       " 3812,\n",
       " 3813,\n",
       " 3816,\n",
       " 3818,\n",
       " 3820,\n",
       " 3822,\n",
       " 3823,\n",
       " 3825,\n",
       " 3829,\n",
       " 3830,\n",
       " 3831,\n",
       " 3834,\n",
       " 3836,\n",
       " 3837,\n",
       " 3841,\n",
       " 3842,\n",
       " 3845,\n",
       " 3846,\n",
       " 3847,\n",
       " 3848,\n",
       " 3849,\n",
       " 3850,\n",
       " 3851,\n",
       " 3852,\n",
       " 3853,\n",
       " 3854,\n",
       " 3861,\n",
       " 3866,\n",
       " 3868,\n",
       " 3871,\n",
       " 3877,\n",
       " 3881,\n",
       " 3885,\n",
       " 3887,\n",
       " 3888,\n",
       " 3889,\n",
       " 3890,\n",
       " 3891,\n",
       " 3892,\n",
       " 3902,\n",
       " 3904,\n",
       " 3905,\n",
       " 3906,\n",
       " 3907,\n",
       " 3908,\n",
       " 3914,\n",
       " 3919,\n",
       " 3924,\n",
       " 3928,\n",
       " 3929,\n",
       " 3930,\n",
       " 3931,\n",
       " 3933,\n",
       " 3937,\n",
       " 3938,\n",
       " 3939,\n",
       " 3946,\n",
       " 3947,\n",
       " 3949,\n",
       " 3955,\n",
       " 3956,\n",
       " 3957,\n",
       " 3958,\n",
       " 3960,\n",
       " 3967,\n",
       " 3968,\n",
       " 3979,\n",
       " 3980,\n",
       " 3981,\n",
       " 3983,\n",
       " 3990,\n",
       " 3992,\n",
       " 3999,\n",
       " 4001,\n",
       " 4005,\n",
       " 4007,\n",
       " 4008,\n",
       " 4012,\n",
       " 4018,\n",
       " 4020,\n",
       " 4023,\n",
       " 4025,\n",
       " 4037,\n",
       " 4038,\n",
       " 4040,\n",
       " 4041,\n",
       " 4042,\n",
       " 4043,\n",
       " 4046,\n",
       " 4047,\n",
       " 4049,\n",
       " 4050,\n",
       " 4054,\n",
       " 4055,\n",
       " 4058,\n",
       " 4059,\n",
       " 4061,\n",
       " 4062,\n",
       " 4063,\n",
       " 4064,\n",
       " 4067,\n",
       " 4068,\n",
       " 4069,\n",
       " 4072,\n",
       " 4075,\n",
       " 4076,\n",
       " 4078,\n",
       " 4082,\n",
       " 4083,\n",
       " 4084,\n",
       " 4085,\n",
       " 4086,\n",
       " 4087,\n",
       " 4091,\n",
       " 4094,\n",
       " 4095,\n",
       " 4099,\n",
       " 4103,\n",
       " 4114,\n",
       " 4115,\n",
       " 4116,\n",
       " 4118,\n",
       " 4119,\n",
       " 4121,\n",
       " 4123,\n",
       " 4126,\n",
       " 4127,\n",
       " 4128,\n",
       " 4130,\n",
       " 4131,\n",
       " 4135,\n",
       " 4136,\n",
       " 4140,\n",
       " 4142,\n",
       " 4143,\n",
       " 4144,\n",
       " 4145,\n",
       " 4147,\n",
       " 4148,\n",
       " 4149,\n",
       " 4151,\n",
       " 4152,\n",
       " 4153,\n",
       " 4155,\n",
       " 4159,\n",
       " 4160,\n",
       " 4161,\n",
       " 4165,\n",
       " 4166,\n",
       " 4168,\n",
       " 4169,\n",
       " 4170,\n",
       " 4173,\n",
       " 4174,\n",
       " 4176,\n",
       " 4177,\n",
       " 4179,\n",
       " 4182,\n",
       " 4184,\n",
       " 4186,\n",
       " 4189,\n",
       " 4193,\n",
       " 4194,\n",
       " 4198,\n",
       " 4199,\n",
       " 4200,\n",
       " 4202,\n",
       " 4203,\n",
       " 4204,\n",
       " 4207,\n",
       " 4208,\n",
       " 4210,\n",
       " 4211,\n",
       " 4218,\n",
       " 4219,\n",
       " 4221,\n",
       " 4222,\n",
       " 4223,\n",
       " 4225,\n",
       " 4226,\n",
       " 4230,\n",
       " 4232,\n",
       " 4234,\n",
       " 4242,\n",
       " 4244,\n",
       " 4245,\n",
       " 4247,\n",
       " 4250,\n",
       " 4251,\n",
       " 4254,\n",
       " 4255,\n",
       " 4257,\n",
       " 4258,\n",
       " 4263,\n",
       " 4265,\n",
       " 4267,\n",
       " 4270,\n",
       " 4271,\n",
       " 4273,\n",
       " 4274,\n",
       " 4283,\n",
       " 4284,\n",
       " 4286,\n",
       " 4288,\n",
       " 4289,\n",
       " 4293,\n",
       " 4295,\n",
       " 4296,\n",
       " 4297,\n",
       " 4298,\n",
       " 4299,\n",
       " 4302,\n",
       " 4305,\n",
       " 4308,\n",
       " 4312,\n",
       " 4313,\n",
       " 4316,\n",
       " 4317,\n",
       " 4318,\n",
       " 4319,\n",
       " 4322,\n",
       " 4323,\n",
       " 4324,\n",
       " 4326,\n",
       " 4328,\n",
       " 4338,\n",
       " 4339,\n",
       " 4342,\n",
       " 4343,\n",
       " 4344,\n",
       " 4345,\n",
       " 4346,\n",
       " 4350,\n",
       " 4351,\n",
       " 4353,\n",
       " 4358,\n",
       " 4359,\n",
       " 4363,\n",
       " 4365,\n",
       " 4366,\n",
       " 4367,\n",
       " 4368,\n",
       " 4369,\n",
       " 4370,\n",
       " 4371,\n",
       " 4373,\n",
       " 4374,\n",
       " 4377,\n",
       " 4379,\n",
       " 4380,\n",
       " 4384,\n",
       " 4386,\n",
       " 4387,\n",
       " 4388,\n",
       " 4391,\n",
       " 4393,\n",
       " 4394,\n",
       " 4397,\n",
       " 4399,\n",
       " 4401,\n",
       " 4404,\n",
       " 4406,\n",
       " 4408,\n",
       " 4410,\n",
       " 4411,\n",
       " 4412,\n",
       " 4417,\n",
       " 4424,\n",
       " 4426,\n",
       " 4427,\n",
       " 4428,\n",
       " 4430,\n",
       " 4432,\n",
       " 4433,\n",
       " 4435,\n",
       " 4436,\n",
       " 4437,\n",
       " 4439,\n",
       " 4441,\n",
       " 4444,\n",
       " 4445,\n",
       " 4452,\n",
       " 4453,\n",
       " 4454,\n",
       " 4455,\n",
       " 4456,\n",
       " 4460,\n",
       " 4461,\n",
       " 4467,\n",
       " 4471,\n",
       " 4475,\n",
       " 4476,\n",
       " 4477,\n",
       " 4480,\n",
       " 4482]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_indexs_in_cluster_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 0.02654177904481197,\n",
       " 2: 0,\n",
       " 3: 0,\n",
       " 4: 0,\n",
       " 5: 0,\n",
       " 6: 0,\n",
       " 7: 0.005984284250031998,\n",
       " 8: 0.009681510181692755,\n",
       " 9: 0}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_doc_topic(docs[2], topic_words_dict, topic_words_prob_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_in_cluster = []\n",
    "for index in doc_indexs_in_cluster_0:\n",
    "    topics_probs = predict_doc_topic(docs[index], topic_words_dict, topic_words_prob_dict)\n",
    "    topics_in_cluster.append(max(topics_probs.items(), key=operator.itemgetter(1))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 6, 7, 8, 9}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(topics_in_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_counter = {}\n",
    "for i in range(n_components):\n",
    "    topic_counter[i] = 0\n",
    "for topic in topics_in_cluster:\n",
    "    topic_counter[topic] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 179, 1: 407, 2: 7, 3: 1, 4: 0, 5: 0, 6: 4, 7: 67, 8: 212, 9: 38}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
